{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('random50posts.csv')\n",
    "ids = df['Post ID'].astype(str).tolist()\n",
    "\n",
    "with open('dataset.json',  'r') as fj:\n",
    "    dataset = json.load(fj)\n",
    "\n",
    "docs = [dataset[x] for x in ids]\n",
    "\n",
    "steps = [\n",
    "    'plugin usage', 'functionality implementation', 'software configuration', 'device configuration',\n",
    "    'plugin usage', 'API usage', 'device deployment', 'software configuration',\n",
    "    'math comprehension', 'OS migration', 'framework comparison', 'version migration',\n",
    "    'runtime', 'OS configuration'\n",
    "]\n",
    "\n",
    "high_level_steps = ['debug', 'implementation',\n",
    "                    'configuration', 'comprehension']\n",
    "\n",
    "details = [\n",
    "    'marker-based AR', 'text recognition', 'camera usage', 'simulator usage',\n",
    "    'video capture', 'video player', 'gesture', 'object detection',\n",
    "    'object movement', 'animation', 'async', 'file saving',\n",
    "    'AR with video', 'matrix analysis', 'device specific development', 'sensor usage',\n",
    "    'controller usage', 'video streaming', 'asynchronous event', 'dimension measurement',\n",
    "    '3d models', 'spatial tracking', 'speech recognition', ''\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sequence': 'if touching object move to location aframe\\n\\n\\nI am wondering how it would be possible in aframe if the  camera reaches 20 spaces away from the point 0 0 0 to teleport them back to that point. Would something like this be easy to achieve or difficult? How can I achieve this.\\n\\n\\n', 'labels': ['functionality implementation', 'device configuration', 'framework comparison', 'device deployment', 'version migration', 'software configuration', 'software configuration', 'API usage', 'runtime', 'plugin usage', 'plugin usage', 'math comprehension', 'OS migration', 'OS configuration'], 'scores': [0.13313549757003784, 0.12298337370157242, 0.10798491537570953, 0.10708724707365036, 0.09249809384346008, 0.06384246051311493, 0.06384246051311493, 0.06333138793706894, 0.059896692633628845, 0.05506926774978638, 0.05506926774978638, 0.0340128131210804, 0.021560292690992355, 0.01968619041144848]}, {'sequence': 'Location based AR with Three.js (+ React) - camera configuration?\\n\\n\\nI want to augment the image of a stationary webcam with location based markers. This is to be added to an existing React app that uses three.js (through react-three-fiber) in other parts already, so these technologies are to be reused.\\n\\n\\nWhile it is quite eays to calculate the position of the markers (locations known) relative to the camera (location known), I\\'m struggling with the configuration of the camera in order to get a good visual match between \"real\" object and AR marker.\\n\\n\\nI have created a codesandbox with an artificial example that illustrates the challenge.\\n\\n\\nHere\\'s my attempt at configuring the camera:\\n\\n\\nconst camera = {\\n    position: [0, 1.5, 0],\\n    fov: 85,\\n    near: 0.005,\\n    far: 1000\\n};\\n\\nconst bearing = 109;  // degrees\\n\\n<Canvas camera={camera}>\\n    <Scene bearing={bearing}/>\\n</Canvas>\\n\\n\\n\\nFurther down in the scene component I’m rotating the camera according to the bearing of the webcam like so:\\n\\n\\n...\\n\\nconst rotation = { x: 0, y: bearing * -1, z: 0 };\\ncamera.rotation.x = (rotation.x * Math.PI) / 180;\\ncamera.rotation.y = (rotation.y * Math.PI) / 180;\\ncamera.rotation.z = (rotation.z * Math.PI) / 180;\\n\\n...\\n\\n\\n\\nAny tips/thoughts on how to get that camera configured for a good match of three.js boxes and real life objects?\\n\\n\\n', 'labels': ['device configuration', 'functionality implementation', 'software configuration', 'software configuration', 'framework comparison', 'device deployment', 'API usage', 'math comprehension', 'plugin usage', 'plugin usage', 'version migration', 'runtime', 'OS configuration', 'OS migration'], 'scores': [0.08536627888679504, 0.08488203585147858, 0.08171843737363815, 0.08171843737363815, 0.07977937906980515, 0.07393506169319153, 0.07219401746988297, 0.07023180276155472, 0.0694664865732193, 0.0694664865732193, 0.0683610588312149, 0.05864744260907173, 0.05342685431241989, 0.05080612376332283]}, {'sequence': 'Running Oculus Unity Sample Framework\\n\\n\\nAre there some pre-requisites to running the Oculus Sample Framework in Unity?  I start with a new project, add the package from the store, and that\\'s as far as I get.  51 compiler errors, mainly relating to missing OVR* namespaces.\\n\\n\\nAlso \"AssetImporter is referencing an asset from the previous import. This should not happen.\".  I thought the idea was that a sample framework would just work?\\n\\n\\nI\\'m running Unity 2018.2.11f1.\\n\\n\\n', 'labels': ['framework comparison', 'software configuration', 'software configuration', 'functionality implementation', 'runtime', 'API usage', 'version migration', 'device configuration', 'plugin usage', 'plugin usage', 'device deployment', 'OS configuration', 'math comprehension', 'OS migration'], 'scores': [0.10890079289674759, 0.08381135761737823, 0.08381135761737823, 0.08336763083934784, 0.08296359330415726, 0.07061351835727692, 0.06531921029090881, 0.06369543820619583, 0.06321056187152863, 0.06321056187152863, 0.06259234994649887, 0.06061380356550217, 0.05414748936891556, 0.05374237895011902]}, {'sequence': \"access local pc filesystem with hololens\\n\\n\\nI'm trying to find a way to access a local PCs file system with a microsoft hololens on the same network. For example. I want to be able to open a txt document on the hololens and save it to my PC. I can't seem to find any relatable documentation online. Basically all I want to do is use the Hololens as a monitor to my PC. Any and all help is appreciated.\\n\\n\\n\", 'labels': ['device configuration', 'functionality implementation', 'OS configuration', 'device deployment', 'software configuration', 'software configuration', 'API usage', 'framework comparison', 'OS migration', 'runtime', 'version migration', 'math comprehension', 'plugin usage', 'plugin usage'], 'scores': [0.12028419226408005, 0.10704788565635681, 0.09449794888496399, 0.08901708573102951, 0.08781562745571136, 0.08781562745571136, 0.05925702676177025, 0.05803434178233147, 0.05506091192364693, 0.054110102355480194, 0.0501815564930439, 0.0462377592921257, 0.04532000795006752, 0.04532000795006752]}, {'sequence': 'Aframe React dynamic string value\\n\\n\\nJust trying to get my head around using react and aframe together with the spread operator etc. \\n\\n\\nSo I have an Entity using Meshline\\n\\n\\n<Entity meshline=\"lineWidth: 20; path: -2 -1 0, 0 -2 0, 2 -1; color: #E20049\" />\\n\\n\\n\\nI\\'m trying to make that path string dynamic with 2 objects i.e. something like\\n\\n\\nbuttonConfig.position = {\\n    x:config.position.x-3,\\n    y:config.position.y+3,\\n    z:config.position.z,\\n}\\n\\nlineX = {\\n    lineWidth : 20,\\n    path: {...config.position,...buttonConfig.position},\\n    color: \\'#ffffff\\',\\n}\\n\\n\\n\\nBut that\\'s obviously not working because I get a \\n\\n\\n\\nTypeError: value.split is not a function\\n\\n\\n\\nHow do I turn that object into a string of values via AFRAME/React? Or am I trying to be too clever here and should just build the string? \\n\\n\\n', 'labels': ['functionality implementation', 'API usage', 'software configuration', 'software configuration', 'device configuration', 'framework comparison', 'plugin usage', 'plugin usage', 'math comprehension', 'runtime', 'version migration', 'device deployment', 'OS configuration', 'OS migration'], 'scores': [0.09641778469085693, 0.08506420254707336, 0.0784585028886795, 0.0784585028886795, 0.07770802825689316, 0.07718926668167114, 0.07018391788005829, 0.07018391788005829, 0.06886662542819977, 0.06513085216283798, 0.06476156413555145, 0.06421537697315216, 0.05347790941596031, 0.04988354444503784]}, {'sequence': 'How to scale and position a SCNNode that has a SCNSkinner attached?\\n\\n\\nUsing SceneKit, I\\'m loading a very simple .dae file consisting of a large cylinder with three associated bones. I want to scale the cylinder down and position it on the ground. Here\\'s the code\\n\\n\\npublic class MyNode: SCNNode {\\n\\n    public convenience init() {\\n        self.init()\\n\\n        let scene = SCNScene(named: \"test.dae\")\\n\\n        let cylinder = (scene?.rootNode.childNode(withName: \"Cylinder\", recursively: true))!\\n\\n        let scale: Float = 0.1\\n        cylinder.scale = SCNVector3Make(scale, scale, scale)\\n        cylinder.position = SCNVector3(0, scale, 0)\\n\\n        self.addChildNode(cylinder)\\n    }\\n}\\n\\n\\n\\nThis doesn\\'t work; the cylinder is still huge when I view it. The only way I can get the code to work is to remove associated SCNSKinner.\\n\\n\\ncylinder.skinner = nil\\n\\n\\n\\nWhy does this happen and how can I properly scale and position the model, bones and all?\\n\\n\\n', 'labels': ['functionality implementation', 'device configuration', 'API usage', 'framework comparison', 'device deployment', 'software configuration', 'software configuration', 'math comprehension', 'runtime', 'plugin usage', 'plugin usage', 'version migration', 'OS configuration', 'OS migration'], 'scores': [0.10272974520921707, 0.08561122417449951, 0.08208873867988586, 0.07418789714574814, 0.07385674118995667, 0.07241306453943253, 0.07241306453943253, 0.07086439430713654, 0.07018450647592545, 0.06542003154754639, 0.06542003154754639, 0.057330239564180374, 0.05636877566576004, 0.051111459732055664]}, {'sequence': 'RealityKit is making my iPhone thermalStatus go crazy with no apparent reason\\n\\n\\nI have the following demo app, In which I set an ARWorldTrackingConfiguration over my RealityKit.\\n\\n\\nI also use plane detection.\\n\\n\\nWhen a plane is detected, I add the ability to \"Fire\" a rectangle on to the plane with a simple square collision box.\\n\\n\\nAfter about 100 squares, the app thermalStatus changes to serious and my frame rate goes down to 30fps.\\n\\n\\nFor the life of me, I can\\'t understand why 100 simple shapes in an RealityKit world, with no special textures or even collision events will cause this.\\n\\n\\nDoes anyone have any idea?\\n\\n\\nPS1: Running this on an iPhone XS, which should be able to perform better according to HW specifications.\\n\\n\\nPS2: Adding the code below\\n\\n\\n\\nimport UIKit\\nimport RealityKit\\nimport ARKit\\n\\nlet material = SimpleMaterial(color: .systemPink, isMetallic: false)\\nvar sphere: MeshResource = MeshResource.generatePlane(width: 0.1, depth: 0.1)\\nvar box = ShapeResource.generateBox(width: 0.1, height: 0.03, depth: 0.1)\\nvar ballEntity = ModelEntity(mesh: sphere, materials: [material])\\nlet collider = CollisionComponent(\\n  shapes: [box],\\n  mode: .trigger\\n)\\n\\nclass ViewController: UIViewController {\\n\\n  @IBOutlet var arView: ARView!\\n\\n  @IBOutlet weak var button: UIButton!\\n\\n  override func viewDidLoad() {\\n    super.viewDidLoad()\\n\\n    let configuration = ARWorldTrackingConfiguration()\\n    configuration.planeDetection = [.vertical]\\n    configuration.worldAlignment = .camera\\n\\n    // Add the box anchor to the scene\\n    configuration.frameSemantics.remove(.bodyDetection)\\n    configuration.frameSemantics.remove(.personSegmentation)\\n    configuration.frameSemantics.remove(.personSegmentationWithDepth)\\n\\n    arView.renderOptions.insert(.disableCameraGrain)\\n    arView.renderOptions.insert(.disableGroundingShadows)\\n    arView.renderOptions.insert(.disableHDR)\\n    arView.renderOptions.insert(.disableMotionBlur)\\n    arView.renderOptions.insert(.disableFaceOcclusions)\\n    arView.renderOptions.insert(.disableDepthOfField)\\n    arView.renderOptions.insert(.disablePersonOcclusion)\\n\\n    configuration.planeDetection = [.vertical, .horizontal]\\n\\n    arView.debugOptions = [.showAnchorGeometry, .showStatistics]\\n\\n    let gesture = UITapGestureRecognizer(target: self,\\n                                         action: #selector(self.tap(_:)))\\n    arView.addGestureRecognizer(gesture)\\n\\n    arView.session.run(configuration, options: [ .resetSceneReconstruction ])\\n  }\\n\\n  @objc func tap(_ sender: UITapGestureRecognizer) {\\n    let point: CGPoint = sender.location(in: arView)\\n\\n    guard let query = arView.makeRaycastQuery(from: point,\\n                                          allowing: .existingPlaneGeometry,\\n                                          alignment: .vertical) else {\\n      return\\n    }\\n\\n    let result = arView.session.raycast(query)\\n    guard let raycastResult = result.first else { return }\\n\\n\\n    let anchor = AnchorEntity(raycastResult: raycastResult)\\n    var ballEntity = ModelEntity(mesh: sphere, materials: [material])\\n    ballEntity.collision = collider\\n    anchor.addChild(ballEntity)\\n\\n\\n    arView.scene.anchors.append(anchor)\\n  }\\n\\n\\n  @IBAction func removePlaneDebugging(_ sender: Any) {\\n    if arView.debugOptions.contains(.showAnchorGeometry) {\\n      arView.debugOptions.remove(.showAnchorGeometry)\\n      button.setTitle(\"Display planes\", for: .normal)\\n      return\\n    }\\n\\n    button.setTitle(\"Remove planes\", for: .normal)\\n    arView.debugOptions.insert(.showAnchorGeometry)\\n  }\\n}\\n\\n\\n\\nCan anyone please assist?\\n\\n\\n', 'labels': ['functionality implementation', 'API usage', 'plugin usage', 'plugin usage', 'device configuration', 'framework comparison', 'software configuration', 'software configuration', 'device deployment', 'runtime', 'math comprehension', 'OS configuration', 'version migration', 'OS migration'], 'scores': [0.07310467958450317, 0.07273367792367935, 0.07187868654727936, 0.07187868654727936, 0.07184208929538727, 0.07156059145927429, 0.07155685871839523, 0.07155685871839523, 0.07148343324661255, 0.0710504949092865, 0.07068397849798203, 0.07060696184635162, 0.07059956341981888, 0.06946337223052979]}, {'sequence': '\\'Plugin \"Google Sceneform Tools (Beta)\" is incompatible (supported only in IntelliJ IDEA)\\' [closed]\\n\\n\\n\\n\\n\\n\\n\\n\\nClosed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.\\n                        \\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\\n\\n\\nClosed last year.\\n\\n\\n\\n\\n\\n\\n\\n                        Improve this question\\n                    \\n\\n\\n\\n\\n\\nI am getting an error Plugin \"Google Sceneform Tools (Beta)\" is incompatible (supported only in IntelliJ IDEA)**. I installed Google Sceneform Tools (Beta) plugin from plugins and after restarting the error stated above is shown. I am currently using Android Studio 4.1\\n\\n\\n', 'labels': ['plugin usage', 'plugin usage', 'software configuration', 'software configuration', 'functionality implementation', 'API usage', 'framework comparison', 'OS configuration', 'runtime', 'device configuration', 'device deployment', 'math comprehension', 'version migration', 'OS migration'], 'scores': [0.10609167069196701, 0.10609167069196701, 0.08162058144807816, 0.08162058144807816, 0.07443489134311676, 0.06849342584609985, 0.06768167018890381, 0.06300455331802368, 0.06294476240873337, 0.06241586431860924, 0.05773145705461502, 0.05709778890013695, 0.05667778104543686, 0.05409327894449234]}, {'sequence': \"How can Unity distort the scene according to the customized VR lenses?\\n\\n\\nI want to design VR lenses for customized VR boxes and I want to develop Android apps in Unity but I cannot figure out how to change the distortion of the image on the screen accordingly.\\n\\n\\nimage distorted for VR\\n\\n\\nSource\\n\\n\\nAll sources I can find about this issue are created about 5 years ago.\\n\\n\\nThis mentions Vertex-Displacement Lens Correction can be done with Cardboard SDK for Unity.\\nThe SDK contains a CG including file titled CardboardDistortion.cginc.\\n\\n\\nThis file contains a method which will convert a world-space vertex into inverse-lens distorted screen space ('lens space'), using the Brown–Conrady model for radial distortion correction.\\n\\n\\nIs there a simpler way to do VR Distortion Correction in Unity?\\nCan Unity distort the scene by changing some coefficients?\\n\\n\\nAre there any other alternative programs to solve that?\\n\\n\\nAnd also are there any alternatives to Google Cardboard SDK to develop VR apps on android phones?\\n\\n\\nSome of the video player apps enables users to change some settings like this in order to set the perfect angle and distortion coefficients (e.g. Deo VR). Can it be done visually in Unity?\\n\\n\\n\", 'labels': ['software configuration', 'software configuration', 'functionality implementation', 'device configuration', 'API usage', 'plugin usage', 'plugin usage', 'OS configuration', 'device deployment', 'framework comparison', 'runtime', 'version migration', 'OS migration', 'math comprehension'], 'scores': [0.08130747824907303, 0.08130747824907303, 0.08074142783880234, 0.07813061773777008, 0.07357578724622726, 0.07176560908555984, 0.07176560908555984, 0.06963131576776505, 0.06952335685491562, 0.06939712911844254, 0.06821393966674805, 0.06646066904067993, 0.059550173580646515, 0.05862932652235031]}, {'sequence': \"How to tell if you're facing a specific CLLocation in iPhone app\\n\\n\\nSo I'm making an Augmented Reality app and I'm a little unsure how to tell, given your current location and heading information and a second location, if you're actually facing that location.  I think it has to do with a specific part of the CLLocation Heading but I'm a little unsure.  Any help would be awesome, thanks a lot everyone\\n\\n\\n\", 'labels': ['device configuration', 'device deployment', 'functionality implementation', 'software configuration', 'software configuration', 'framework comparison', 'OS configuration', 'version migration', 'API usage', 'runtime', 'OS migration', 'plugin usage', 'plugin usage', 'math comprehension'], 'scores': [0.1695549637079239, 0.09943751245737076, 0.09498593211174011, 0.09322580695152283, 0.09322580695152283, 0.08595584332942963, 0.0681358054280281, 0.05991353467106819, 0.05007690191268921, 0.046169083565473557, 0.036248430609703064, 0.03448273241519928, 0.03448273241519928, 0.03410491719841957]}, {'sequence': 'ARToolkit Unity - Camera blank (only black) when Playing\\n\\n\\n\\nI want to create Augmented Reality using Unity and ARToolkit but when done then play the camera was blank. I follow this tutorial from Youtube\\n\\n\\n', 'labels': ['functionality implementation', 'software configuration', 'software configuration', 'framework comparison', 'device configuration', 'plugin usage', 'plugin usage', 'API usage', 'runtime', 'device deployment', 'OS configuration', 'version migration', 'math comprehension', 'OS migration'], 'scores': [0.14909157156944275, 0.10222966969013214, 0.10222966969013214, 0.0870906412601471, 0.08266351372003555, 0.07130370289087296, 0.07130370289087296, 0.06504059582948685, 0.061460111290216446, 0.05920650437474251, 0.04828881844878197, 0.04597942531108856, 0.03014739230275154, 0.023964764550328255]}, {'sequence': \"How to use POSE in ARCore?\\n\\n\\nThe ARCore documentation defines Pose as :\\n\\n\\n\\nPose represents an immutable rigid transformation from one coordinate space to another. As provided from all ARCore APIs, Poses always describe the transformation from the object's local coordinate space to the world coordinate space. The transformation is defined using a quaternion rotation about the origin followed by a translation.\\n\\n\\n\\nWhat is object's local coordinate space and world coordinate space?\\n\\n\\n\", 'labels': ['API usage', 'functionality implementation', 'software configuration', 'software configuration', 'device configuration', 'plugin usage', 'plugin usage', 'framework comparison', 'math comprehension', 'device deployment', 'runtime', 'OS configuration', 'version migration', 'OS migration'], 'scores': [0.4600656032562256, 0.07180824130773544, 0.053434811532497406, 0.053434811532497406, 0.04567250981926918, 0.04542246088385582, 0.04542246088385582, 0.04527351260185242, 0.039545342326164246, 0.03420833498239517, 0.03365891054272652, 0.02999526634812355, 0.02745598927140236, 0.014601776376366615]}, {'sequence': \"How to control hololens app with pc?\\n\\n\\nI'm newbies for hololens and learning app develop for hololens. I want to have this effect. For example, I click a button in unity editor,then a movie start to play in hololens. May be I can use something like tcp...But I don't know how to start...\\n\\n\\n\", 'labels': ['functionality implementation', 'device configuration', 'software configuration', 'software configuration', 'framework comparison', 'OS configuration', 'device deployment', 'plugin usage', 'plugin usage', 'API usage', 'version migration', 'runtime', 'OS migration', 'math comprehension'], 'scores': [0.14195172488689423, 0.12783648073673248, 0.11116519570350647, 0.11116519570350647, 0.08114246279001236, 0.0619632862508297, 0.05955135077238083, 0.058401498943567276, 0.058401498943567276, 0.05621105432510376, 0.03797377645969391, 0.036314744502305984, 0.03145334869623184, 0.02646852470934391]}, {'sequence': '3D model does not appear when rendering\\n\\n\\nI\\'m using Vuforia to place a 3D model on an image target. I have created a common C++ solution to work on both Android and iOS. It works on Android, but I can\\'t get the 3D model to appear in iOS. It tracks the image target perfectly, but there\\'s no sign of the 3D model. The 3D model I\\'m using can be found here.\\n\\n\\nThis is how I\\'m doing:\\n\\n\\nThis method is called by Vuforia every time the screen needs to be rendered:\\n\\n\\n- (void)renderFrameQCAR\\n{\\n    [self setFramebuffer];\\n    [[ObjectController getInstance] getObjectInstance]->renderFrame();\\n    [self presentFramebuffer];\\n}\\n\\n\\n\\nThis is the setFramebuffer method (Objective-C++):\\n\\n\\n- (void)setFramebuffer\\n{\\n    if (context) {\\n        [EAGLContext setCurrentContext:context];\\n\\n        if (!defaultFramebuffer) {\\n            [self performSelectorOnMainThread:@selector(createFramebuffer) withObject:self waitUntilDone:YES];\\n        }\\n\\n#ifdef USE_OPENGL1\\n        glBindFramebufferOES(GL_FRAMEBUFFER_OES, defaultFramebuffer);\\n#else\\n        glBindFramebuffer(GL_FRAMEBUFFER, defaultFramebuffer);\\n#endif\\n    }\\n}\\n\\n\\n\\nThis is the renderFrame method (C++):\\n\\n\\nvoid IDRObject::renderFrame()\\n{\\n    // Clear color and depth buffer\\n    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);\\n\\n    // Get the state from QCAR and mark the beginning of a rendering section\\n    QCAR::State state = QCAR::Renderer::getInstance().begin();\\n\\n    // Explicitly render the Video Background\\n    QCAR::Renderer::getInstance().drawVideoBackground();\\n\\n#ifdef DEVICE_OPENGL_1\\n    // Set GL11 flags:\\n    glEnableClientState(GL_VERTEX_ARRAY);\\n    glEnableClientState(GL_NORMAL_ARRAY);\\n    glEnableClientState(GL_TEXTURE_COORD_ARRAY);\\n\\n    glEnable(GL_TEXTURE_2D);\\n    glDisable(GL_LIGHTING);\\n\\n#endif\\n\\n    glEnable(GL_DEPTH_TEST);\\n\\n    // We must detect if background reflection is active and adjust the culling direction.\\n    // If the reflection is active, this means the post matrix has been reflected as well,\\n    // therefore standard counter clockwise face culling will result in \"inside out\" models.\\n    glEnable(GL_CULL_FACE);\\n    glCullFace(GL_BACK);\\n    if(QCAR::Renderer::getInstance().getVideoBackgroundConfig().mReflection == QCAR::VIDEO_BACKGROUND_REFLECTION_ON)\\n        glFrontFace(GL_CW);  //Front camera\\n    else\\n        glFrontFace(GL_CCW);   //Back camera\\n\\n    SampleUtils::checkGlError(\"gl start setup stuff\");\\n    // Did we find any trackables this frame?\\n    for(int tIdx = 0; tIdx < state.getNumTrackableResults(); tIdx++)\\n    {\\n        // Get the trackable:\\n        const QCAR::TrackableResult* result = state.getTrackableResult(tIdx);\\n        const QCAR::Trackable& trackable = result->getTrackable();\\n        QCAR::Matrix44F modelViewMatrix = QCAR::Tool::convertPose2GLMatrix(result->getPose());\\n\\n        // Choose the texture based on the target name:\\n        int textureIndex;\\n        if (strcmp(trackable.getName(), \"chips\") == 0)\\n        {\\n            textureIndex = 0;\\n        }\\n        else if (strcmp(trackable.getName(), \"stones\") == 0)\\n        {\\n            textureIndex = 1;\\n        }\\n        else\\n        {\\n            textureIndex = 2;\\n        }\\n\\n        const Texture* const thisTexture = textures[textureIndex];\\n\\n#ifdef DEVICE_OPENGL_1\\n        // Load projection matrix:\\n        glMatrixMode(GL_PROJECTION);\\n        glLoadMatrixf(projectionMatrix.data);\\n\\n        // Load model view matrix:\\n        glMatrixMode(GL_MODELVIEW);\\n        glLoadMatrixf(modelViewMatrix.data);\\n        glTranslatef(0.f, 0.f, kObjectScale);\\n        glScalef(kObjectScale, kObjectScale, kObjectScale);\\n\\n        // Draw object:\\n        glBindTexture(GL_TEXTURE_2D, thisTexture->mTextureID);\\n        glTexCoordPointer(2, GL_FLOAT, 0, (const GLvoid*) &teapotTexCoords[0]);\\n        glVertexPointer(3, GL_FLOAT, 0, (const GLvoid*) &teapotVertices[0]);\\n        glNormalPointer(GL_FLOAT, 0,  (const GLvoid*) &teapotNormals[0]);\\n        glDrawElements(GL_TRIANGLES, NUM_TEAPOT_OBJECT_INDEX, GL_UNSIGNED_SHORT,\\n                   (const GLvoid*) &teapotIndices[0]);\\n\\n#else\\n\\n\\n        QCAR::Matrix44F modelViewProjection;\\n\\n        SampleUtils::translatePoseMatrix(0.0f, 0.0f, kObjectScale, &modelViewMatrix.data[0]);\\n        SampleUtils::scalePoseMatrix(kObjectScale, kObjectScale, kObjectScale, &modelViewMatrix.data[0]);\\n        SampleUtils::multiplyMatrix(&projectionMatrix.data[0], &modelViewMatrix.data[0], &modelViewProjection.data[0]);\\n\\n        glUseProgram(shaderProgramID);\\n\\n        glVertexAttribPointer(vertexHandle, 3, GL_FLOAT, GL_FALSE, 0, (const GLvoid*) &teapotVertices[0]);\\n        glVertexAttribPointer(normalHandle, 3, GL_FLOAT, GL_FALSE, 0, (const GLvoid*) &teapotNormals[0]);\\n        glVertexAttribPointer(textureCoordHandle, 2, GL_FLOAT, GL_FALSE, 0, (const GLvoid*) &teapotTexCoords[0]);\\n\\n        glEnableVertexAttribArray(vertexHandle);\\n        glEnableVertexAttribArray(normalHandle);\\n        glEnableVertexAttribArray(textureCoordHandle);\\n\\n        glActiveTexture(GL_TEXTURE0);\\n        glBindTexture(GL_TEXTURE_2D, thisTexture->mTextureID);\\n\\n        glUniformMatrix4fv(mvpMatrixHandle, 1, GL_FALSE, (GLfloat*)&modelViewProjection.data[0] );\\n        glUniform1i(texSampler2DHandle, 0 /*GL_TEXTURE0*/);\\n        glDrawElements(GL_TRIANGLES, NUM_TEAPOT_OBJECT_INDEX, GL_UNSIGNED_SHORT, (const GLvoid*) &teapotIndices[0]);\\n\\n        LOG(\"Tracking awesome targets.\\\\n\");\\n        SampleUtils::checkGlError(\"ImageTargets renderFrame\\\\n\");\\n#endif\\n\\n    }\\n\\n    glDisable(GL_DEPTH_TEST);\\n    glDisable(GL_CULL_FACE);\\n\\n#ifdef DEVICE_OPENGL_1\\n    glDisable(GL_TEXTURE_2D);\\n    glDisableClientState(GL_VERTEX_ARRAY);\\n    glDisableClientState(GL_NORMAL_ARRAY);\\n    glDisableClientState(GL_TEXTURE_COORD_ARRAY);\\n#else\\n    glDisableVertexAttribArray(vertexHandle);\\n    glDisableVertexAttribArray(normalHandle);\\n    glDisableVertexAttribArray(textureCoordHandle);\\n#endif\\n\\n    QCAR::Renderer::getInstance().end();\\n}\\n\\n\\n\\nAnd the last presentFrameBuffer (Objective-C++):\\n\\n\\n- (BOOL)presentFramebuffer\\n{\\n    BOOL success = FALSE;\\n\\n    if (context) {\\n        [EAGLContext setCurrentContext:context];\\n\\n#ifdef USE_OPENGL1\\n        glBindRenderbufferOES(GL_RENDERBUFFER_OES, colorRenderbuffer);\\n#else\\n        glBindRenderbuffer(GL_RENDERBUFFER, colorRenderbuffer);\\n#endif\\n\\n        success = [context presentRenderbuffer:GL_RENDERBUFFER];\\n    }\\n\\n    return success;\\n}\\n\\n\\n\\n', 'labels': ['functionality implementation', 'framework comparison', 'API usage', 'device configuration', 'software configuration', 'software configuration', 'device deployment', 'OS configuration', 'plugin usage', 'plugin usage', 'version migration', 'runtime', 'OS migration', 'math comprehension'], 'scores': [0.08029410988092422, 0.07470057159662247, 0.07446613162755966, 0.07383466511964798, 0.07355891913175583, 0.07355891913175583, 0.07254154980182648, 0.0707947164773941, 0.0695265606045723, 0.0695265606045723, 0.06848397850990295, 0.06810494512319565, 0.0658780187368393, 0.06473048031330109]}, {'sequence': 'MicrosoftDevicePortalWrapper UploadFileAsync error\\n\\n\\nI develop UWP app. This application connects Hololens and Desktop. I use MicrosoftDevicePortalWrapper nuget packages. \\nhttps://github.com/Microsoft/WindowsDevicePortalWrapper/blob/master/GettingStarted.md\\nhttps://github.com/Microsoft/WindowsDevicePortalWrapper\\n\\n\\nUploadFileAsync method always throw this exception \"This IRandomAccessStream does not support the GetInputStream method because it requires cloning and this stream does not support cloning.\"\\nI use everything as described in the documentation. Always throws this bug when you enter the filePath.\\n\\n\\nHere my code\\n\\n\\npublic async Task<bool> SaveFiles(string userName, string password, string url, string filePath)\\n{\\n    try\\n    {\\n        DevicePortal portal = new DevicePortal(new DefaultDevicePortalConnection(url, userName, password));\\n        await portal.ConnectAsync();\\n        await portal.UploadFileAsync(\"LocalAppData\", filePath, \"LocalState\", \"4ab51c0b-cfe9-4548-a3b0-ca8db55bbfd2_1.0.0.0_x86__3ygapy8kah9j6\");\\n        return true;\\n    }\\n    catch (COMException)\\n    {\\n        return false;\\n    }\\n}\\n\\n\\n\\n', 'labels': ['functionality implementation', 'API usage', 'device configuration', 'device deployment', 'software configuration', 'software configuration', 'plugin usage', 'plugin usage', 'OS configuration', 'framework comparison', 'runtime', 'OS migration', 'version migration', 'math comprehension'], 'scores': [0.09076736867427826, 0.08383065462112427, 0.07923002541065216, 0.07919386029243469, 0.0766473188996315, 0.0766473188996315, 0.07030054181814194, 0.07030054181814194, 0.06960942596197128, 0.06773065775632858, 0.06451183557510376, 0.06341370195150375, 0.061280712485313416, 0.046535998582839966]}, {'sequence': 'AR.js Basic Scene: Image.png appears on the UI\\n\\n\\nSorry in advance, I am a complete beginner regarding programming, but I cant seem to find an answer to my problem.\\nI made this basic setup from AR.js:\\n\\n\\n<!doctype HTML>\\n<html>\\n<script src=\"https://aframe.io/releases/1.2.0/aframe.min.js\"></script>\\n<script src=\"https://rawgit.com/donmccurdy/aframe-extras/master/dist/aframe-extras.loaders.min.js\"></script>\\n<script src=\"https://cdn.rawgit.com/jeromeetienne/AR.js/1.5.0/aframe/build/aframe-ar.js\"> </script>\\n  <a-assets>\\n  <a-asset-item id=\"rub\" src=\"models/rubcube.gltf\"></a-asset-item>\\n  </a-assets>\\n    <body style=\\'margin : 0px; overflow: hidden;\\'>\\n    \\n    <a-scene arjs=\"debugUIEnabled: false;\">\\n        \\n          <a-marker-camera preset=\"hiro\"> \\n            <a-entity \\n                      gltf-model=\"#rub\"\\n                      scale=\" 0.5 0.5 0.5\">\\n            </a-entity>\\n          <a-entity camera></a-entity>\\n      </a-marker-camera>\\n    </a-scene>\\n  </body>\\n</html>\\n\\n\\n\\nThe problem I get sooner or later with every AR code is that the session starts not with a camera but with a Image. The Image is the png file that is used for the display of the gltf model. When I move the marker in front of the camera the Image disappears and I can see the camera. The marker tracking itself is working.\\n\\n\\n', 'labels': ['functionality implementation', 'device configuration', 'API usage', 'framework comparison', 'device deployment', 'software configuration', 'software configuration', 'version migration', 'math comprehension', 'plugin usage', 'plugin usage', 'runtime', 'OS configuration', 'OS migration'], 'scores': [0.0844523161649704, 0.07944150269031525, 0.07734650373458862, 0.07648827135562897, 0.07626336812973022, 0.07542870938777924, 0.07542870938777924, 0.07098392397165298, 0.06817429512739182, 0.06770771741867065, 0.06770771741867065, 0.06289442628622055, 0.061793796718120575, 0.05588872730731964]}, {'sequence': 'How to move Camera in Rajawali Virtual Reality framework for android?\\n\\n\\nThis is the link of Rajawali VR example : \\n\\n\\nhttps://github.com/MasDennis/RajawaliVR\\n\\n\\nI want to add a feature that the user can move the camera around in the scene just like a FPS game such as Counter-Strike.\\n\\n\\nso,I write this code in the RajawaliVRExampleActivity\\'s onCreate method :\\n\\n\\n/**this is the method*/ \\n@Override\\npublic void onCreate(Bundle savedInstanceState) {\\n    requestWindowFeature(Window.FEATURE_NO_TITLE);\\n    getWindow().setFlags(\\n            WindowManager.LayoutParams.FLAG_FULLSCREEN\\n                    | WindowManager.LayoutParams.FLAG_KEEP_SCREEN_ON,\\n            WindowManager.LayoutParams.FLAG_FULLSCREEN\\n                    | WindowManager.LayoutParams.FLAG_KEEP_SCREEN_ON);\\n\\n    super.onCreate(savedInstanceState);\\n\\n    setRequestedOrientation(ActivityInfo.SCREEN_ORIENTATION_LANDSCAPE);\\n\\n\\n    mRenderer = new RajawaliVRExampleRenderer(this);\\n    mRenderer.setSurfaceView(mSurfaceView);\\n    setRenderer(mRenderer);\\n\\n    /**this is what I write*/\\n    final Handler handler = new Handler();\\n    Runnable runnable = new Runnable() {\\n        int i = 1;\\n        @Override\\n        public void run() {\\n            mRenderer.getCurrentCamera().setZ(i--);\\n            Log.i(\"i = \", \"\"+i);\\n            handler.postDelayed(this,2000);\\n        }\\n    };\\n    handler.postDelayed(runnable,2000);\\n}\\n\\n\\n\\nI think it will move forward every two second after i run the program,but there\\'s a difference between the left side and right side.\\n\\n\\nSo,who can tell me how to move the camera correctly?\\n\\n\\n', 'labels': ['functionality implementation', 'framework comparison', 'software configuration', 'software configuration', 'device configuration', 'API usage', 'device deployment', 'OS configuration', 'plugin usage', 'plugin usage', 'runtime', 'version migration', 'math comprehension', 'OS migration'], 'scores': [0.0799122154712677, 0.07759515941143036, 0.07628802210092545, 0.07628802210092545, 0.07434792071580887, 0.07261057198047638, 0.07193847000598907, 0.06980785727500916, 0.06935326009988785, 0.06935326009988785, 0.06843322515487671, 0.0675557479262352, 0.06360702216625214, 0.06290920078754425]}, {'sequence': 'A-Frame 0.3.0 not loading OBJs correctly - works in 0.2.0\\n\\n\\nWhen I updated scenes to use A-Frame 0.3.0, my OBJs no longer load correctly.\\n\\n\\nAfter changing the UVs on some models to not be negative (something which is legal OBJ) they started working in Firefox, but show up as white in Chrome and Safari. Chrome and Safari also ignore any direct color assignments -- \"color: red\" for instance. I\\'m not using any MTL files - just the geometry.\\n\\n\\nBefore I go down the long road of hand editing OBJs until stuff works - were there any changes to A-Frame for 0.3.0 or in three.js that might be biting me? Like: no longer liking exponent notation in the OBJs? Needing to have something specific in the header? etc.\\n\\n\\nthanks,\\n\\n\\n', 'labels': ['software configuration', 'software configuration', 'framework comparison', 'functionality implementation', 'plugin usage', 'plugin usage', 'device configuration', 'API usage', 'OS configuration', 'version migration', 'device deployment', 'runtime', 'math comprehension', 'OS migration'], 'scores': [0.09494234621524811, 0.09494234621524811, 0.08868487179279327, 0.08689507842063904, 0.0796472355723381, 0.0796472355723381, 0.0755242183804512, 0.07317855209112167, 0.06355731189250946, 0.05944225192070007, 0.05726578086614609, 0.05588463321328163, 0.048617929220199585, 0.041770193725824356]}, {'sequence': \"Image does not contain a definition for 'PIXEL_FORMAT'\\n\\n\\nI've tried testing the c# code for a QR code reader in this answer here\\n\\n\\nI just copy pasted the code into a C# script and placed it in the ARCamera prefab just like how the KDelli said but I've been getting this error:\\n\\n\\n\\nerror CS0117: 'Image' does not contain a definition for 'PIXEL_FORMAT'\\n\\n\\n\\nfrom these two lines:\\n\\n\\nprivate IEnumerator InitializeCamera() {\\n    var isFrameFormatSet = CameraDevice.Instance.SetFrameFormat(Image.PIXEL_FORMAT.RGB888, true);\\n}\\nprivate void Update() {\\n    var cameraFeed = CameraDevice.Instance.GetCameraImage(Image.PIXEL_FORMAT.RGB888);\\n}\\n\\n\\n\\nI think I just missed something like a using code/library or it might be due to the code being old that this error arise.\\n\\n\\n\", 'labels': ['device configuration', 'functionality implementation', 'API usage', 'device deployment', 'software configuration', 'software configuration', 'framework comparison', 'plugin usage', 'plugin usage', 'runtime', 'math comprehension', 'version migration', 'OS configuration', 'OS migration'], 'scores': [0.10436522215604782, 0.09376932680606842, 0.08193319290876389, 0.08136788755655289, 0.08010994642972946, 0.08010994642972946, 0.07883671671152115, 0.06841151416301727, 0.06841151416301727, 0.06084682419896126, 0.05757506564259529, 0.05226714909076691, 0.05093325674533844, 0.04106239974498749]}, {'sequence': \"Move object left and right in ARKit session\\n\\n\\nHow to move an object left and right in ARKit scene without changing its Y-axis, if its in the air then it doesn't change that and just move left and right in screen? \\n\\n\\n\", 'labels': ['functionality implementation', 'device configuration', 'framework comparison', 'device deployment', 'API usage', 'software configuration', 'software configuration', 'version migration', 'runtime', 'plugin usage', 'plugin usage', 'math comprehension', 'OS configuration', 'OS migration'], 'scores': [0.2646215260028839, 0.1366637945175171, 0.1045607179403305, 0.08044903725385666, 0.07033469527959824, 0.05902264639735222, 0.05902264639735222, 0.049475204199552536, 0.042218055576086044, 0.040335796773433685, 0.040335796773433685, 0.024015026167035103, 0.01672758348286152, 0.012217501178383827]}, {'sequence': 'Unable to get correct coordinates for QR code, ARKit - Swift\\n\\n\\nI am trying to place an object above a QR code in swift. I am able to detect the QR code however the location is wrong for the placement of the box. I don\\'t really understand how the placement works. I know how to place objects down on planes. Do I need to relate the detected planes somehow with where it says it detects the QR code? Any information on how this SCNVector columns thing works would be appreciated as well haha. Also if CIDector is out dated and there is a new method.\\n\\n\\nHere is a snippet of detecting the QRCode and placing the box:\\nvar discoveredQRCodes = String\\n\\n\\nfunc session(_ session: ARSession, didUpdate frame: ARFrame) {\\n        \\n        //print(\"Updated\")\\n        if time != 0.5 {\\n            return\\n        }\\n    \\n        DispatchQueue.global(qos: .background).async {\\n\\n        let image = CIImage(cvPixelBuffer: frame.capturedImage)\\n        let detector = CIDetector(ofType: CIDetectorTypeQRCode, context: nil, options: nil)\\n        let features = detector!.features(in: image)\\n\\n        \\n            \\n        for feature in features as! [CIQRCodeFeature] {\\n            if !self.discoveredQRCodes.contains(feature.messageString!) {\\n                self.discoveredQRCodes.append(feature.messageString!)\\n                let url = URL(string: feature.messageString!)\\n                let position = SCNVector3(frame.camera.transform.columns.3.x,\\n                                          frame.camera.transform.columns.3.y,\\n                                          frame.camera.transform.columns.3.z)\\n//                add3DModel(fromURL: url!, toPosition: getPositionBasedOnQRCode(frame: frame, position: \"df\"))\\n                print(position)\\n                print(url)\\n                \\n                DispatchQueue.main.async {\\n                let boxNode = SCNNode()\\n                boxNode.geometry = SCNBox(width: 0.04, height: 0.04, length: 0.04, chamferRadius: 0.002)\\n                boxNode.geometry?.firstMaterial?.diffuse.contents = UIColor.green\\n                boxNode.position = position\\n                boxNode.name = \"node\"\\n                self.arView.scene.rootNode.addChildNode(boxNode)\\n                }\\n                //add3dInstance(fromURL: url!, toPosition: position)\\n            }\\n        }\\n            \\n        }\\n            \\n        \\n}\\n\\n\\n\\nHere is an image of the result:\\n\\n\\n\\n\\n\\nHere is some debug output:\\nSCNVector3(x: 0.023941405, y: 0.040143043, z: 0.056782123)\\n\\n\\n', 'labels': ['functionality implementation', 'API usage', 'device configuration', 'software configuration', 'software configuration', 'framework comparison', 'device deployment', 'OS configuration', 'version migration', 'math comprehension', 'plugin usage', 'plugin usage', 'runtime', 'OS migration'], 'scores': [0.07726197689771652, 0.07389102876186371, 0.07326304912567139, 0.07316631823778152, 0.07316631823778152, 0.07201163470745087, 0.07117661833763123, 0.07080700993537903, 0.07040467858314514, 0.07032066583633423, 0.06935276091098785, 0.06935276091098785, 0.06841841340065002, 0.06740681827068329]}, {'sequence': 'Execute \"LaunchUriAsync\" from a click of a HoloLens UI Button\\n\\n\\nI\\'m trying to do a \"LaunchUriAsync\" and I have trouble trying to get to it to launch from a ButtonPress event. I\\'ve tried to edit from the guide at Microsoft Developer \"Call Remote Assist from our Hololens\" as I do not require the GestureRecognizer for it.  \\n\\n\\nAre there any solutions/workaround for this ?\\n\\n\\nHere is the portion of the code I\\'m referencing:\\n\\n\\nusing System.Collections;\\nusing System.Collections.Generic;\\nusing UnityEngine;\\nusing System.Threading.Tasks;\\nusing UnityEngine.XR.WSA.Input;\\n\\npublic class CallApp : MonoBehaviour\\n{\\n      // Use this for initialization\\n    async void Start()\\n    {\\n\\n    }\\n\\n    public void CallActive()\\n    {\\n#if ENABLE_WINMD_SUPPORT\\n        string uriToLaunch = @\"ms-voip-video:?contactids=2914d36d-34f5-4f86-8196-52f4e53cf384\";\\n        Debug.Log(\"LaunchUriAsync: \" + uriToLaunch);\\n\\n        System.Uri uri = new System.Uri(uriToLaunch);\\n        await Windows.ApplicationModel.Core.CoreApplication.MainView.CoreWindow.Dispatcher.RunAsync(Windows.UI.Core.CoreDispatcherPriority.Normal, () =>\\n        {\\n            // Work done on the UI thread\\n            LaunchURI(uri).ConfigureAwait(false).GetAwaiter().GetResult();\\n        });\\n#endif\\n    }\\n\\n\\n    public async Task LaunchURI(System.Uri uri)\\n    {\\n#if ENABLE_WINMD_SUPPORT\\n        // Launch the URI\\n        try\\n        {\\n            var success = await Windows.System.Launcher.LaunchUriAsync(uri);\\n\\n            if (success)\\n            {\\n                Debug.Log(\"URI launched\");\\n            }\\n            else\\n            {\\n                Debug.Log(\"URI launch failed\");\\n            }\\n        }\\n        catch (Exception ex)\\n        {\\n            Debug.Log(ex.Message);\\n        }\\n#endif\\n    }\\n\\n}\\n\\n\\n\\n', 'labels': ['functionality implementation', 'API usage', 'software configuration', 'software configuration', 'OS configuration', 'device configuration', 'device deployment', 'OS migration', 'framework comparison', 'runtime', 'version migration', 'plugin usage', 'plugin usage', 'math comprehension'], 'scores': [0.07745577394962311, 0.07394220679998398, 0.07332457602024078, 0.07332457602024078, 0.07277920842170715, 0.07151097804307938, 0.07136107981204987, 0.07122081518173218, 0.0710889920592308, 0.06948249787092209, 0.06914354115724564, 0.06888533383607864, 0.06888533383607864, 0.06759516894817352]}, {'sequence': 'Linux configuration ARToolKit, make error\\n\\n\\nI would like to compile ARToolKit source code on Linux, download the source code, and in accordance with the ARToolKit document, configuration GLUT, OpenGL, libjpeg other libraries.\\n\\n\\nGo to the ARToolKit directory and type ./Configer\\nConfiger information image.\\n\\n\\nEnter the make command,The error occurs.\\n\\n\\nWhat are the causes of these errors? How can I solve? thanks.\\n\\n\\n', 'labels': ['software configuration', 'software configuration', 'OS configuration', 'functionality implementation', 'framework comparison', 'device configuration', 'API usage', 'plugin usage', 'plugin usage', 'device deployment', 'OS migration', 'runtime', 'version migration', 'math comprehension'], 'scores': [0.18465693295001984, 0.18465693295001984, 0.09648270905017853, 0.061044689267873764, 0.056340496987104416, 0.056326743215322495, 0.05303752049803734, 0.050562672317028046, 0.050562672317028046, 0.048616550862789154, 0.04714195802807808, 0.041875772178173065, 0.039798445999622345, 0.028895946219563484]}, {'sequence': 'How to set 3d models in arCore plane without tapping\\n\\n\\nI want to set a 3d model on a random part of the ARcore immediately when the plane is detected. Not sure of which method to use to setup.\\n\\n\\naddOnUpdateListener(this::onUpdate);\\n\\n private void onUpdate(FrameTime frameTime) {\\n    if (numOfModels > 0) return;\\n    modelLoader1 = new ModelLoader(weakReference);\\n    Frame frame = arFragment.getArSceneView().getArFrame();\\n    Collection<Plane> planes = frame.getUpdatedTrackables(Plane.class);\\n    for (Plane plane : planes) {\\n        if (plane.getTrackingState() == TrackingState.TRACKING) {\\n            addObject(Uri.parse(\"andy_dance.sfb\"));\\n            break;\\n        }\\n    }\\n}\\n\\nprivate void addObject(Uri model) {\\n    Frame frame = arFragment.getArSceneView().getArFrame();\\n    android.graphics.Point pt = getScreenCenter();\\n    List<HitResult> hits;\\n    if (frame != null) {\\n        hits = frame.hitTest(pt.x, pt.y);\\n        for (HitResult hit : hits) {\\n            Trackable trackable = hit.getTrackable();\\n            if (trackable instanceof Plane &&\\n                    ((Plane) trackable).isPoseInPolygon(hit.getHitPose())) {\\n                modelLoader1.loadModel(hit.createAnchor(), model);\\n                break;\\n\\n            }\\n        }\\n    }\\n}\\n\\npublic void loadModel(Anchor anchor, Uri uri) {\\n    if (owner.get() == null) {\\n        Log.d(TAG, \"Activity is null.  Cannot load model.\");\\n        return;\\n    }\\n    ModelRenderable.builder()\\n            .setSource(owner.get(), uri)\\n            .build()\\n            .handle((renderable, throwable) -> {\\n                MainActivity activity = owner.get();\\n                if (activity == null) {\\n                    return null;\\n                } else if (throwable != null) {\\n                    activity.onException(throwable);\\n                } else {\\n                    activity.addNodeToScene(anchor, renderable);\\n                }\\n                return null;\\n            });\\n\\n    return;\\n}\\n\\npublic void addNodeToScene(Anchor anchor, ModelRenderable renderable) {\\n    numOfModels++;\\n    AnchorNode anchorNode = new AnchorNode(anchor);\\n    TransformableNode node = new TransformableNode(arFragment.getTransformationSystem());\\n    node.setRenderable(renderable);\\n    node.setParent(anchorNode);\\n    node.setLocalPosition(new Vector3(0f, 0f, 0f));\\n    modelLoader1.setNumofLivesModel0(2);\\n    arFragment.getArSceneView().getScene().addChild(anchorNode);\\n\\n    setNodeListener(node, anchorNode, modelLoader1);\\n\\n\\n\\nSo far I am able to populate the screen with one model but at the center of the screen, we would like to set the model in the corner if possible for now\\n\\n\\n', 'labels': ['functionality implementation', 'API usage', 'device configuration', 'framework comparison', 'software configuration', 'software configuration', 'plugin usage', 'plugin usage', 'device deployment', 'math comprehension', 'runtime', 'version migration', 'OS configuration', 'OS migration'], 'scores': [0.09722607582807541, 0.08002922683954239, 0.07810737192630768, 0.07711797207593918, 0.07529379427433014, 0.07529379427433014, 0.07124312222003937, 0.07124312222003937, 0.0710778459906578, 0.0667160376906395, 0.06621624529361725, 0.06443499028682709, 0.057244136929512024, 0.04875623807311058]}, {'sequence': 'How I resize an object in ARCore?\\n\\n\\nI show the 3D object in ArFragment. So I put .obj file and .mtl file at sampledata folder. And I right click on obj file, and select Import Sceneform Asset to add .sfa / .sfb file.\\n\\n\\nSo I can show the 3d object when I mark the image, but the object is too big. \\n\\n\\nThis is my .sfa file detail\\n\\n\\n{\\n   bound_relative_root: {\\n      x: 0.5,\\n      y: 0,\\n      z: 0.5,\\n   },\\n   materials: [\\n      {\\n         name: \"Material.001\",\\n         parameters: [\\n            {\\n               baseColor: null,\\n            },\\n            {\\n               baseColorTint: [\\n                  0.80000000000000004,\\n                  0.80000000000000004,\\n                  0.80000000000000004,\\n                  1,\\n               ],\\n            },\\n            {\\n               metallic: 1,\\n            },\\n            {\\n               roughness: 0.120695,\\n            },\\n            {\\n               opacity: null,\\n            },\\n         ],\\n         source: \"build/sceneform_sdk/default_materials/obj_material.sfm\",\\n      },\\n   ],\\n   model: {\\n      attributes: [\\n         \"Position\",\\n         \"TexCoord\",\\n         \"Orientation\",\\n      ],\\n      collision: {},\\n      file: \"sampledata/dongbaek.obj\",\\n      name: \"dongbaek\",\\n      recenter: \"root\",\\n      scale: 0.200000\\n   },\\n   version: \"0.52:1\",\\n}\\n\\n\\n\\nI think it can resize by scale part, but I change the value, it dosen\\'t change. same size\\n\\n\\nSo How I can resize 3d object?\\n\\n\\nIs there any problem at add 3d object file to make .sfa / .sfb file?(Import Sceneform Asset)\\n\\n\\nIf you know about it, please help me.\\n\\n\\n', 'labels': ['functionality implementation', 'framework comparison', 'software configuration', 'software configuration', 'API usage', 'device configuration', 'device deployment', 'plugin usage', 'plugin usage', 'version migration', 'OS configuration', 'math comprehension', 'runtime', 'OS migration'], 'scores': [0.08784250169992447, 0.07658553123474121, 0.07573996484279633, 0.07573996484279633, 0.07481646537780762, 0.07302268594503403, 0.07188502699136734, 0.07163781672716141, 0.07163781672716141, 0.06975870579481125, 0.06466745585203171, 0.06423869729042053, 0.06378296762704849, 0.058644283562898636]}, {'sequence': 'How in Unity of action of the button to attach to AR-objects (ImageTarget)?\\n\\n\\nThere are several ImageTargets, and there are three buttons in the interface, when you click, the sound plays (voicing in three languages). How can I attach an action to ImageTarget?\\n\\n\\n', 'labels': ['functionality implementation', 'device configuration', 'framework comparison', 'API usage', 'device deployment', 'software configuration', 'software configuration', 'runtime', 'plugin usage', 'plugin usage', 'version migration', 'math comprehension', 'OS configuration', 'OS migration'], 'scores': [0.25165385007858276, 0.14266228675842285, 0.08970742672681808, 0.08181159198284149, 0.07385890185832977, 0.05489836633205414, 0.05489836633205414, 0.051892220973968506, 0.04763888940215111, 0.04763888940215111, 0.03535987064242363, 0.026780521497130394, 0.02596324123442173, 0.015235650353133678]}, {'sequence': \"Accessing the HoloLens Accelerometer\\n\\n\\nI'm trying to get the HoloLens's accelerometer data for the purpose of movement speed while in a car. \\n\\n\\n\\nI tried using the Windows 'Devices' namespace to access their 'Sensors.Accelerometer' object, but that's inexplicably incompatible with the HoloLens ... a Windows Device. \\nI was able to cheat by calculating the speed of the camera object in Unity, but that only works relative to the headset wearer (when I test the app in the passenger seat of a car, I just the speed at which I move my head around). \\nSome time ago I'd found a Github repo from MS about HoloLens sensor streams, but I can no longer find it (Dear Future Me, clone EVERYTHING).   \\n\\n\\n\\nDoes anyone know if there's another way I can get the accelerometer data? Either in C# or C++. \\n\\n\\nPS I have Research Mode and MS's demo app on my HoloLens and I've been analyzing that code for answers too. Didn't want anyone to think I show up here looking for a magic bullet!\\n\\n\\n\", 'labels': ['device configuration', 'software configuration', 'software configuration', 'functionality implementation', 'device deployment', 'OS configuration', 'API usage', 'runtime', 'plugin usage', 'plugin usage', 'framework comparison', 'math comprehension', 'version migration', 'OS migration'], 'scores': [0.09653028100728989, 0.08872715383768082, 0.08872715383768082, 0.08004435151815414, 0.07933351397514343, 0.07609613984823227, 0.07011683285236359, 0.06247994303703308, 0.0622919537127018, 0.0622919537127018, 0.0621473602950573, 0.0600292831659317, 0.05689609423279762, 0.05428803339600563]}, {'sequence': 'Contextual type \\'Any\\' cannot be used with dictionary literal\\n\\n\\nI\\'m getting that error, I\\'m new to swift and I would like to create a json out of the ARKit\\n\\n\\nlet jsonObject: [String: Any] = [\\n            \"imageName\": imageName,\\n            \"timeStamp\": currentFrame.timestamp,\\n            \"cameraPos\": dictFromVector3(positionFromTransform(currentFrame.camera.transform)),\\n            \"cameraEulerAngle\": dictFromVector3(currentFrame.camera.eulerAngles),\\n            \"cameraTransform\": arrayFromTransform(currentFrame.camera.transform),\\n            \"cameraIntrinsics\": arrayFromTransform(currentFrame.camera.intrinsics),\\n            \"imageResolution\": [\\n                \"width\": currentFrame.camera.imageResolution.width,\\n                \"height\": currentFrame.camera.imageResolution.height\\n            ],\\n            \"lightEstimate\": currentFrame.lightEstimate?.ambientIntensity,\\n            \"ARPointCloud\": [\\n                \"count\": currentFrame.rawFeaturePoints?.count,\\n                \"points\": arrayFromPointCloud(currentFrame.rawFeaturePoints)\\n            ]\\n        ]\\n\\n\\n\\n', 'labels': ['framework comparison', 'functionality implementation', 'API usage', 'math comprehension', 'device configuration', 'plugin usage', 'plugin usage', 'software configuration', 'software configuration', 'runtime', 'version migration', 'device deployment', 'OS configuration', 'OS migration'], 'scores': [0.13546481728553772, 0.1343049705028534, 0.1254170536994934, 0.08077836036682129, 0.06865920126438141, 0.06588170677423477, 0.06588170677423477, 0.0602809302508831, 0.0602809302508831, 0.05201658979058266, 0.05074598267674446, 0.0480036661028862, 0.030305901542305946, 0.02197815105319023]}, {'sequence': 'pyOpenGL: memory issue using openvr with pyqt5 (HTC Vive)\\n\\n\\nwe are developing a virtual environment using qt5 and pyopenvr with the HTC Vive. All our scripts are fine and working. On one laptop, however, we all of a sudden keep having an issue.\\n\\n\\nIt\\'s a high-end gaming laptop equipped with a gtx1060 (6gb), so it can\\'t be a real memory problem. Even a complete system reboot with only the required installations and all drivers up to date didn\\'t solve it. It used to work at one point when we first tested the laptop, but now this error keeps recurring:\\n\\n\\nGLError: GLError(\\nerr = 1285,\\ndescription = b\\'Nicht gen\\\\xfcgend Arbeitsspeicher\\',\\nbaseOperation = glRenderbufferStorageMultisample,\\ncArguments = (\\n    GL_RENDERBUFFER,\\n    2,\\n    GL_DEPTH24_STENCIL8,\\n    1512,\\n    1680,\\n))\\n\\n\\n\\n\"Nicht genügend Arbeitsspeicher\" is the German equivalent for out of memory.\\nThis happens even if we are running only the sample \"hello world\"-script of pyopenvr to show the simple color cube. The error is the same when using our scripts. On another laptop, everything works fine.\\n\\n\\nAnyone encountered a similar issue? Any help appreciated!\\n\\n\\n', 'labels': ['functionality implementation', 'device configuration', 'software configuration', 'software configuration', 'device deployment', 'framework comparison', 'OS configuration', 'runtime', 'API usage', 'plugin usage', 'plugin usage', 'version migration', 'OS migration', 'math comprehension'], 'scores': [0.09082353860139847, 0.08425669372081757, 0.08152960240840912, 0.08152960240840912, 0.07560070604085922, 0.07534312456846237, 0.0744122564792633, 0.0720028504729271, 0.07007502019405365, 0.0667932853102684, 0.0667932853102684, 0.05692746862769127, 0.05283743143081665, 0.05107514560222626]}, {'sequence': 'OnPointerEnter is not being called\\n\\n\\nI\\'m trying to animate the button while it is being gazed on. I have got the following code in which I Raycast from a sphere to find the button that it hits.\\n\\n\\nvar eventDataCurrentPosition = new PointerEventData(EventSystem.current);\\n        eventDataCurrentPosition.position = screenPosition;\\n\\nvar results = new List<RaycastResult>();\\n\\nEventSystem.current.RaycastAll(eventDataCurrentPosition, results);\\n\\nforeach (var result in results)\\n{\\n     Debug.Log(result.gameObject.name);\\n}  \\n\\n\\n\\nIn order to animate the button, I\\'m adding the following code to the button. Unfortunately, the OnPointerEnter or ``OnPointerExit is never being called.\\n\\n\\n[RequireComponent(typeof(Button))]\\npublic class InteractiveItem : MonoBehaviour, IPointerEnterHandler, IPointerExitHandler\\n{\\n    public Image  progressImage;\\n    public bool   isEntered = false;\\n    RectTransform rt;\\n    Button        _button;\\n    float         timeElapsed;\\n    Image         cursor;\\n    float         GazeActivationTime = 5;\\n\\n// Use this for initialization\\nvoid Awake()\\n{\\n    _button = GetComponent<Button>();\\n    rt      = GetComponent<RectTransform>();\\n}\\n\\n\\nvoid Update()\\n{\\n    if (isEntered)\\n    {\\n        timeElapsed              += Time.deltaTime;\\n        progressImage.fillAmount =  Mathf.Clamp(timeElapsed / GazeActivationTime, 0, 1);\\n        if (timeElapsed >= GazeActivationTime)\\n        {\\n            timeElapsed = 0;\\n            _button.onClick.Invoke();\\n            progressImage.fillAmount = 0;\\n            isEntered                = false;\\n        }\\n    }\\n    else\\n    {\\n        timeElapsed = 0;\\n    }\\n}\\n\\n#region IPointerEnterHandler implementation\\n\\npublic void OnPointerEnter(PointerEventData eventData)\\n{\\n    CodelabUtils._ShowAndroidToastMessage(\"entered\");\\n    isEntered = true;\\n}\\n\\n#endregion\\n\\n#region IPointerExitHandler implementation\\n\\npublic void OnPointerExit(PointerEventData eventData)\\n{\\n    CodelabUtils._ShowAndroidToastMessage(\"exit\");\\n\\n    isEntered                = false;\\n    progressImage.fillAmount = 0;\\n}\\n\\n#endregion\\n}\\n\\n\\n\\nam I missing something ? or is there any other way of achieving this?\\n\\n\\n', 'labels': ['functionality implementation', 'API usage', 'math comprehension', 'plugin usage', 'plugin usage', 'runtime', 'device deployment', 'device configuration', 'software configuration', 'software configuration', 'framework comparison', 'version migration', 'OS configuration', 'OS migration'], 'scores': [0.07557490468025208, 0.07376385480165482, 0.07278605550527573, 0.07231339812278748, 0.07231339812278748, 0.07182522118091583, 0.07156204432249069, 0.07135012745857239, 0.0704716369509697, 0.0704716369509697, 0.070069819688797, 0.06955583393573761, 0.06954996287822723, 0.06839209794998169]}, {'sequence': \"AR ODG application for conference calls\\n\\n\\nI'm researching AR frameworks in order to select the best option for developing conference call/ meeting application for ODG glasses. \\n\\n\\nI got only a few directions for selecting a framework:\\n\\n\\nPerformance of video streaming (capturing and encoding) must be watched closely  to avoid overheating and excessive power consumption,\\n\\n\\nShould support extended tracking and \\n\\n\\nVideo capturing should not be frame by frame.\\n\\n\\nI have no experience with AR field in general, and I would really appreciate if you can let me know your opinion or to give me some guidance on how to choose the best-fitted framework.\\n\\n\\n\", 'labels': ['framework comparison', 'functionality implementation', 'API usage', 'software configuration', 'software configuration', 'device configuration', 'plugin usage', 'plugin usage', 'math comprehension', 'runtime', 'device deployment', 'version migration', 'OS configuration', 'OS migration'], 'scores': [0.11498185992240906, 0.09801706671714783, 0.09456514567136765, 0.07554079592227936, 0.07554079592227936, 0.06911434233188629, 0.06835165619850159, 0.06835165619850159, 0.06700390577316284, 0.06281400471925735, 0.06186414882540703, 0.053415700793266296, 0.05069660395383835, 0.03974230960011482]}, {'sequence': 'OpenCV trying to integrate ARtoolkit with OpenCV\\n\\n\\nI am new to C++, openCV and Artoolkit\\nI am trying to build a motion tracker devices\\nright now, I am following the tutorial\\n\\n\\nhttps://artoolkit.org/blog/2016/05/opencv-with-artoolkit\\n\\n\\nHowever I meet some problem when I trying to implement this on SimpleTest on the Linux machine.\\n\\n\\nThe error I get is like this:\\n\\n\\n\"clang++ -c -O3 -fPIC -march=core2 -DHAVE_NFT=1 -I/usr/include/x86_64-linux-gnu -pthread -I/usr/include/gstreamer-0.10 -I/usr/include/glib-2.0 -I/usr/lib/x86_64-linux-gnu/glib-2.0/include -I/usr/include/libxml2 -I../../include simpleTest.c\\n\\nclang: warning: treating \\'c\\' input as \\'c++\\' when in C++ mode, this behavior is deprecated In file included from simpleTest.c:79: In file included from ../../include/linux-x86_64/opencv2/opencv.hpp:59:/usr/include/opencv2/contrib/contrib.hpp:273:23: error: no template named\\n  \\'vector\\'; did you mean \\'std::vector\\'?\"\\n\\n\\n\\nsimpleTest code\\n\\n\\nI added line like this\\n\\n\\n#include <linux-x86_64/opencv2/opencv.hpp>\\n#include <linux-x86_64/opencv2/opencv_modules.hpp>\\nusing namespace std;\\nusing namespace cv;\\n\\n\\n\\nin the make file:\\nI add some this:\\n\\n\\nLIBS= -lARgsub -lARvideo -lAR -lARICP -lAR -lglut -lGLU -lGL -lX11 -lm -lpthread -ljpeg -pthread -lgstreamer-0.10 -lgobject-2.0 -lgmodule-2.0 -lgthread-2.0 -lxml2 -lglib-2.0 -ldc1394 -lraw1394 -lopencv_shape -lopencv_stitching -lopencv_objdetect -lopencv_superres -lopencv_videostab -lopencv_calib3d -lopencv_features2d -lopencv_highgui -lopencv_videoio -lopencv_imgcodecs -lopencv_video -lopencv_photo -lopencv_ml -lopencv_imgproc -lopencv_flann -lopencv_viz -lippicv -lopencv_core \\n\\n\\n\\nand \\n\\n\\nCC = clang++\\n\\n\\n\\n', 'labels': ['OS configuration', 'OS migration', 'device deployment', 'device configuration', 'functionality implementation', 'API usage', 'framework comparison', 'software configuration', 'software configuration', 'plugin usage', 'plugin usage', 'math comprehension', 'version migration', 'runtime'], 'scores': [0.07666615396738052, 0.07571987062692642, 0.07276792079210281, 0.07269287109375, 0.07151379436254501, 0.0708370953798294, 0.07080738246440887, 0.07066985219717026, 0.07066985219717026, 0.07035131007432938, 0.07035131007432938, 0.06969404220581055, 0.06866391748189926, 0.06859462708234787]}, {'sequence': 'Can I make a universal app using HTML that runs on Hololens?\\n\\n\\nI believe these statement are true:\\n\\n\\n1) All Universal Apps Work As Holograms\\n\\n\\n2) Universal Apps can be built using HTML/JS\\n\\n\\nDoes this mean I can build a holographic universal app using web technologies? For example a holographic visualizations dashboard in D3.js?\\n\\n\\n', 'labels': ['functionality implementation', 'framework comparison', 'device configuration', 'API usage', 'device deployment', 'runtime', 'software configuration', 'software configuration', 'plugin usage', 'plugin usage', 'version migration', 'math comprehension', 'OS configuration', 'OS migration'], 'scores': [0.21295113861560822, 0.08705011755228043, 0.07784376293420792, 0.07746929675340652, 0.07530070841312408, 0.0733911469578743, 0.07041378319263458, 0.07041378319263458, 0.049351561814546585, 0.049351561814546585, 0.04505838081240654, 0.03907168656587601, 0.03892897441983223, 0.033403974026441574]}, {'sequence': 'How to place 3D-model on the top of Face Landmarks, like a face-filter app?\\n\\n\\nI am using two libraries: OpenCV for computer vision functions and OpenSceneGraph for computer graphics functions. Because, the main purpose of the software is augmented reality. The main aim of the software is to create a face-filter like the one in Snapchat, and so far I have done with the facial landmarks (computer vision part) and loading 3d model inside the camera feed of OpenCV with the help of OpenSceneGraph functions. The problem is that I tried to place the 3d model on the top of face landmarks but it did not work perfectly because the coordinates of the model is different from the OpenCV facial landmarks model.\\nSo, is there a way where I can place the model on the top of facial landmarks perfectly?\\n\\n\\nI have tried to change the position of the model based on the facial landmarks coordinate points but without luck, even if I have divided the number by a factor of 10 or 20, because the coordinate points that come from the facial landmarks are huge compared to the ones from the model\\'s position. Note that the position of the model has three coordinate points which are: x, y and z, while points that come from the facial landmarks are only x and y.\\n\\n\\nmain.cpp\\n\\n\\n#include <iostream>\\n#include <osgViewer/Viewer>\\n#include <osgDB/ReadFile>\\n#include <osg/PositionAttitudeTransform>\\n\\n#include \"opencv2/highgui/highgui.hpp\"\\n#include \"opencv2/core/core.hpp\"\\n#include \"OpenCVFuncs.hpp\"\\n\\n#include \"/home/bardawil/Desktop/OSG-OpenCV-ARDemo/include/BackgroundCamera.h\"\\n#include \"/home/bardawil/Desktop/OSG-OpenCV-ARDemo/include/VirtualCamera.h\"\\n\\nusing namespace cv;\\nusing namespace cv::face;\\nusing namespace std;\\n\\n// Initialization: -\\n\\n    // ** OSG Stuff **\\n    int screenWidth, screenHeight, textureWidth, textureHeight;\\n    // Create viewer\\n    osgViewer::Viewer viewer;\\n    // Main Camera\\n    osg::ref_ptr<osg::Camera>  camera = viewer.getCamera();\\n    // Background-Camera (OpenCV Feed)\\n    BackgroundCamera bgCamera;\\n    // Load glass Model as Example Scene\\n    osg::ref_ptr<osg::Node> glassModel = osgDB::readNodeFile(\"priestene test.obj\");\\n    // Model position initial value\\n    osg::Vec3 modelPosition(0, 100, 10);\\n    // Model scale initial value\\n    osg::Vec3 modelScale(150, 150, 150);\\n\\n    // ** OpenCV Stuff **\\n    // Video Capture initialization (from desktop camera)\\n    cv::VideoCapture cap(0);\\n\\n    Mat gray;\\n\\n    // Load Face Detector\\n    CascadeClassifier faceDetector(\"/home/bardawil/Desktop/OSG-OpenCV-ARDemo/haarcascade_frontalface_alt2.xml\");\\n\\n    // Create an instance of Facemark\\n    Ptr<Facemark> facemark = FacemarkLBF::create();\\n\\n    struct faceParams faceStruct;\\n    // struct eyesLM eyes;\\n\\n\\n\\nint main( int argc, char** argv )\\n{\\n    int count  = 0;\\n    facemark->loadModel(\"/home/bardawil/Desktop/OSG-OpenCV-ARDemo/lbfmodel.yaml\");\\n\\n    screenWidth = 640;\\n    screenHeight = 480;\\n\\n    textureWidth = 640;\\n    textureHeight = 480;\\n\\n    // OSG STUFF\\n    viewer.setUpViewInWindow(50,50,screenWidth,screenHeight);\\n\\n    // Virtual Camera setup\\n    VirtualCamera* vCamera = new VirtualCamera(camera);\\n\\n    // OpenCV camera\\n    osg::Camera* backgroundCamera = bgCamera.createCamera(textureWidth, textureHeight);\\n\\n    osg::Group* glassesGroup = new osg::Group();\\n    // Position of glass\\n    osg::PositionAttitudeTransform* position = new osg::PositionAttitudeTransform();\\n\\n    glassesGroup->addChild(position);\\n    position->addChild(glassModel);\\n\\n    // Set Position of Model\\n    position->setPosition(modelPosition);\\n\\n    // Set Scale of Model\\n    position->setScale(modelScale);\\n\\n    // Create new group node\\n    osg::ref_ptr<osg::Group> group = new osg::Group;\\n    osg::Node* background = backgroundCamera;\\n    osg::Node* foreground = glassesGroup;\\n    background->getOrCreateStateSet()->setRenderBinDetails(1,\"RenderBin\");\\n    foreground->getOrCreateStateSet()->setRenderBinDetails(2,\"RenderBin\");\\n    group->addChild(background);\\n    group->addChild(foreground);\\n    background->getOrCreateStateSet()->setMode(GL_DEPTH_TEST,osg::StateAttribute::OFF);\\n    foreground->getOrCreateStateSet()->setMode(GL_DEPTH_TEST,osg::StateAttribute::ON);\\n\\n    // Add the groud to the viewer\\n    viewer.setSceneData(group.get());\\n\\n    if(!cap.isOpened())\\n    {\\n            std::cout << \"Webcam cannot open!\\\\n\";\\n            return 0;\\n    }\\n\\n\\n    while (!viewer.done())\\n    {\\n        // Refresh Background Image\\n        cv::Mat frame;\\n        faceStruct.frame = frame;\\n        cap.read(frame);\\n        // bgCamera.update(frame);\\n\\n        // Update Virtual Camera (these Coordinates should be determined by some AR-Framework/Functionality)\\n        // They are just updated for demonstration purposes..\\n        // Position Parameters: Roll, Pitch, Heading, X, Y, Z\\n        // vCamera->updatePosition(0, 0, 0, 0, 0, 0);\\n        //osg::notify(osg::WARN)<<\"Angle: \"<<  angleRoll <<std::endl;\\n        vector<Rect> faces;\\n\\n        // // Convert frame to grayscale because\\n        // // faceDetector requires grayscale image.\\n\\n        cvtColor(frame, gray, COLOR_BGR2GRAY);\\n\\n        // // Detect faces\\n        // const int scale = 3;\\n        // cv::Mat frame_gray( cvRound( gray.rows / scale ), cvRound( gray.cols / scale ), CV_8UC1 );\\n        // cv::resize( gray, frame_gray, frame_gray.size() );\\n        faceDetector.detectMultiScale(gray, faces, 1.05 , 6, 0 | CASCADE_SCALE_IMAGE, Size(30, 30));\\n\\n        vector< vector<Point2f> > landmarks;\\n\\n        // // Run landmark detector\\n        bool success = facemark->fit(frame, faces, landmarks);\\n\\n        if(success){\\n            // eyes = drawLandmarks(frame, landmarks[0]);   \\n            vCamera->updatePosition(0, 0, 0, 0, 0, 0);\\n        }\\n        // Display results \\n        // flip(frame, frame, +1);\\n\\n        // vCamera->updatePosition(0, 0, 0, 0, 0, 0);\\n        bgCamera.update(frame);\\n        viewer.frame();\\n    }\\n    return 0;\\n}\\n\\n\\n\\n', 'labels': ['functionality implementation', 'software configuration', 'software configuration', 'OS configuration', 'API usage', 'device configuration', 'OS migration', 'plugin usage', 'plugin usage', 'device deployment', 'version migration', 'framework comparison', 'runtime', 'math comprehension'], 'scores': [0.07479853928089142, 0.07341369241476059, 0.07341369241476059, 0.07295145839452744, 0.07291071861982346, 0.07253610342741013, 0.07167109102010727, 0.07128790766000748, 0.07128790766000748, 0.07033108919858932, 0.07009327411651611, 0.06991975009441376, 0.06843717396259308, 0.06694761663675308]}, {'sequence': \"Pause game OnSteamVR Overlay Active\\n\\n\\nI've been following this guide that covers pause with Steam Overlay:\\n\\n\\nhttps://wiki.unrealengine.com/Pause_Game_On_Steam_Overlay_Active\\n\\n\\nIt works very well. However I cannot find any way to make it work with SteamVR Overlay. My code is almost identical except for class names and adding delegate for broadcasting results. Any idea how can i make it work? I thought that it's exactly the same.\\n\\n\\n\", 'labels': ['functionality implementation', 'software configuration', 'software configuration', 'device configuration', 'runtime', 'API usage', 'device deployment', 'framework comparison', 'plugin usage', 'plugin usage', 'OS configuration', 'version migration', 'math comprehension', 'OS migration'], 'scores': [0.1432240605354309, 0.12003885954618454, 0.12003885954618454, 0.08945203572511673, 0.07232142239809036, 0.0626673772931099, 0.06202223524451256, 0.058298300951719284, 0.05811396986246109, 0.05811396986246109, 0.05157272890210152, 0.042623329907655716, 0.035022519528865814, 0.026490306481719017]}, {'sequence': 'Mixed Reality WebRTC - Screen capturing with GraphicsCapturePicker\\n\\n\\nSetup\\nHey,\\nI\\'m trying to capture my screen and send/communicate the stream via MR-WebRTC. Communication between two PCs or PC with HoloLens worked with webcams for me, so I thought the next step could be streaming my screen. So I took the uwp application that I already had, which worked with my webcam and tried to make things work:\\n\\n\\n\\nUWP App is based on the example uwp app from MR-WebRTC.\\nFor Capturing I\\'m using the instruction from MS about screen capturing via GraphicsCapturePicker.\\n\\n\\n\\nSo now I\\'m stuck in the following situation:\\n\\n\\n\\nI get a frame from the screen capturing, but its type is Direct3D11CaptureFrame. You can see it below in the code snipped.\\nMR-WebRTC takes a frame type I420AVideoFrame (also in a code snipped).\\n\\n\\n\\nHow can I \"connect\" them?\\n\\n\\n\\nI420AVideoFrame wants a frame in the I420A format (YUV 4:2:0).\\nConfiguring the framePool I can set the DirectXPixelFormat, but it has no YUV420.\\nI found this post on so, saying that it its possible.\\n\\n\\n\\nCode Snipped Frame from Direct3D:\\n\\n\\n_framePool = Direct3D11CaptureFramePool.Create(\\n                _canvasDevice,                             // D3D device\\n                DirectXPixelFormat.B8G8R8A8UIntNormalized, // Pixel format\\n                3,                                         // Number of frames\\n                _item.Size);                               // Size of the buffers\\n\\n_session = _framePool.CreateCaptureSession(_item);\\n_session.StartCapture();\\n_framePool.FrameArrived += (s, a) =>\\n{\\n    using (var frame = _framePool.TryGetNextFrame())\\n    {\\n        // Here I would take the Frame and call the MR-WebRTC method LocalI420AFrameReady  \\n    }\\n};\\n\\n\\n\\nCode Snippet Frame from WebRTC:\\n\\n\\n// This is the way with the webcam; so LocalI420 was subscribed to\\n// the event I420AVideoFrameReady and got the frame from there\\n_webcamSource = await DeviceVideoTrackSource.CreateAsync();\\n_webcamSource.I420AVideoFrameReady += LocalI420AFrameReady;\\n\\n// enqueueing the newly captured video frames into the bridge,\\n// which will later deliver them when the Media Foundation\\n// playback pipeline requests them.\\nprivate void LocalI420AFrameReady(I420AVideoFrame frame)\\n    {\\n        lock (_localVideoLock)\\n        {\\n            if (!_localVideoPlaying)\\n            {\\n                _localVideoPlaying = true;\\n\\n                // Capture the resolution into local variable useable from the lambda below\\n                uint width = frame.width;\\n                uint height = frame.height;\\n\\n                // Defer UI-related work to the main UI thread\\n                RunOnMainThread(() =>\\n                {\\n                    // Bridge the local video track with the local media player UI\\n                    int framerate = 30; // assumed, for lack of an actual value\\n                    _localVideoSource = CreateI420VideoStreamSource(\\n                        width, height, framerate);\\n                    var localVideoPlayer = new MediaPlayer();\\n                    localVideoPlayer.Source = MediaSource.CreateFromMediaStreamSource(\\n                        _localVideoSource);\\n                    localVideoPlayerElement.SetMediaPlayer(localVideoPlayer);\\n                    localVideoPlayer.Play();\\n                });\\n            }\\n        }\\n        // Enqueue the incoming frame into the video bridge; the media player will\\n        // later dequeue it as soon as it\\'s ready.\\n        _localVideoBridge.HandleIncomingVideoFrame(frame);\\n    }\\n\\n\\n\\n', 'labels': ['device configuration', 'functionality implementation', 'software configuration', 'software configuration', 'device deployment', 'API usage', 'framework comparison', 'OS configuration', 'math comprehension', 'plugin usage', 'plugin usage', 'version migration', 'OS migration', 'runtime'], 'scores': [0.07290731370449066, 0.07270251959562302, 0.07252074033021927, 0.07252074033021927, 0.07202158868312836, 0.07178309559822083, 0.07177931815385818, 0.07156582921743393, 0.07142149657011032, 0.07105088979005814, 0.07105088979005814, 0.07016507536172867, 0.06937605142593384, 0.06913444399833679]}, {'sequence': 'Can i use LibGDX to program a VR game with the Oculus Rift + touch?\\n\\n\\nCan i use LibGDX to program a VR game with the Oculus Rift + touch? I just got an oculus rift + touch controllers and would like to code a game. I am very familiar with LibGDX and would like to stay using that as the game engine. Is this possible and if so, how? \\n\\n\\n', 'labels': ['device configuration', 'functionality implementation', 'software configuration', 'software configuration', 'API usage', 'device deployment', 'OS configuration', 'framework comparison', 'plugin usage', 'plugin usage', 'runtime', 'version migration', 'OS migration', 'math comprehension'], 'scores': [0.16449302434921265, 0.13095298409461975, 0.1242390125989914, 0.1242390125989914, 0.07694397866725922, 0.07316470146179199, 0.055892299860715866, 0.051693323999643326, 0.0430687852203846, 0.0430687852203846, 0.037036482244729996, 0.03444080799818039, 0.022071657702326775, 0.018695130944252014]}, {'sequence': \"hololens setParent position changes camera lens\\n\\n\\nI am working on using the Hololens Lens Toolkit Master.\\nThe problem is that when you set the SetParent of the lens camera, the camera position of the lens becomes the same as the position of the parent.\\n\\n\\nFor example, if A's position is 0, 0, 0 and B's position is 0, 0, 4, then A.SetParent(B.Transform) would make A's position be 0, 0, -4.\\n\\n\\nThis is also true on Unity Editor.\\n\\n\\nHowever, if you build on hololens and run A.SetParent(B.Transform), the position of A will be 0, 0, 4.\\n\\n\\nI have no idea why this happens ...\\n\\n\\nI want 0, 0, -4 !!\\n\\n\\n\", 'labels': ['functionality implementation', 'software configuration', 'software configuration', 'device configuration', 'runtime', 'device deployment', 'version migration', 'framework comparison', 'API usage', 'OS configuration', 'plugin usage', 'plugin usage', 'math comprehension', 'OS migration'], 'scores': [0.1123422160744667, 0.09306669235229492, 0.09306669235229492, 0.0909283459186554, 0.0747944563627243, 0.07290501892566681, 0.06901349872350693, 0.06848762929439545, 0.06560666114091873, 0.05554591864347458, 0.054628126323223114, 0.054628126323223114, 0.04798625782132149, 0.04700030758976936]}, {'sequence': 'How to get list of Bluetooth devices connected to a Oculus Quest 2 and display the camera?\\n\\n\\nI am trying to get my iPhone 12 front camera to display on a canvas in my Oculus Quest 2 VR headset.\\n\\n\\nMy thought process was to connect my phone to the VR headset via Bluetooth and then in a script get the connected device as an object. Below you can see the methods I have tried already.\\n\\n\\nvar inputDevices = new List<InputDevice>();\\nInputDevices.GetDevices(inputDevices);\\nWebCamDevice[] devices = WebCamTexture.devices;\\nvar b = SystemInfo.deviceType;\\n\\n\\n\\nI am unsure how to get the connected device - and yes I did double check my phone was connected.\\n\\n\\nAfter the device is found I plan to display the phone camera on the canvas in the VR world, so that it can become a mixed reality application.\\n\\n\\nAny help would be greatly appreciated and if you want me to post more info just let me know.\\n\\n\\n', 'labels': ['device configuration', 'device deployment', 'functionality implementation', 'software configuration', 'software configuration', 'API usage', 'OS configuration', 'framework comparison', 'runtime', 'plugin usage', 'plugin usage', 'version migration', 'OS migration', 'math comprehension'], 'scores': [0.10116805136203766, 0.09343507885932922, 0.09228307008743286, 0.08566848188638687, 0.08566848188638687, 0.06916285306215286, 0.06629976630210876, 0.06557616591453552, 0.06494078040122986, 0.06008730083703995, 0.06008730083703995, 0.057682108134031296, 0.049230851233005524, 0.048709798604249954]}, {'sequence': 'What is the difference between Session.getAllTrackables and Frame.getUpdatedTrackables?\\n\\n\\nDo both return all trackables known for now? \\n\\n\\nWhy do we need both? \\n\\n\\nWhen should call which one?\\n\\n\\nSame question is for Session.getAllAnchors and Frame.getUpdatedAnchors.\\n\\n\\n', 'labels': ['framework comparison', 'functionality implementation', 'API usage', 'software configuration', 'software configuration', 'device configuration', 'runtime', 'plugin usage', 'plugin usage', 'device deployment', 'math comprehension', 'version migration', 'OS configuration', 'OS migration'], 'scores': [0.11133664846420288, 0.0898924246430397, 0.08041028678417206, 0.07750497758388519, 0.07750497758388519, 0.07742832601070404, 0.06792645156383514, 0.06727148592472076, 0.06727148592472076, 0.06470974534749985, 0.06307052820920944, 0.06253564357757568, 0.05103836953639984, 0.0420987494289875]}, {'sequence': 'restrict iOS app to A12/A12X Bionic chips\\n\\n\\nApple says (https://developer.apple.com/augmented-reality/arkit/)\\n\\n\\n\"People occlusion and the use of motion capture, simultaneous front and back camera, and multiple face tracking are supported on devices with A12/A12X Bionic chips, ANE, and TrueDepth Camera.\"\\n\\n\\nhow do I restrict my arkit app to those devices only on the appstore ?\\n\\n\\n', 'labels': ['device configuration', 'software configuration', 'software configuration', 'functionality implementation', 'OS configuration', 'device deployment', 'version migration', 'framework comparison', 'API usage', 'OS migration', 'runtime', 'plugin usage', 'plugin usage', 'math comprehension'], 'scores': [0.21007750928401947, 0.1251448094844818, 0.1251448094844818, 0.1110980287194252, 0.08180288225412369, 0.07589781284332275, 0.04135339707136154, 0.040724895894527435, 0.038950491696596146, 0.03501817584037781, 0.030895065516233444, 0.029496196657419205, 0.029496196657419205, 0.024899788200855255]}, {'sequence': 'Shoot problem with Oculus Go after recenter\\n\\n\\ni try to make a VR Game with unity and Oculus Go.\\nYou can shoot with the Oculus Go controller and it works if i dont recenter the camera. If i recenter my Camera, the bullets fly in the wrong direction.\\nFor my controller, i have a line renderer. This works perfect.\\nAnyone has an idea why that is?\\ngunBarrel is an child object of my controller\\nsorry for my bad english!!!\\n\\n\\n        if (OVRInput.GetDown(OVRInput.Button.PrimaryIndexTrigger))\\n        {\\n            GameObject bulletTmp = Instantiate(bullet, gunBarrel.transform.position, Quaternion.identity);\\n            bulletTmp.GetComponent<Rigidbody>().AddForce(gunBarrel.transform.TransformDirection(Vector3.forward) * 150);\\n            bulletTmp.transform.LookAt(gunBarrel.transform.position, Vector3.up);\\n            return true;\\n        }\\n\\n\\n\\n', 'labels': ['functionality implementation', 'device configuration', 'software configuration', 'software configuration', 'API usage', 'device deployment', 'version migration', 'framework comparison', 'OS configuration', 'runtime', 'plugin usage', 'plugin usage', 'math comprehension', 'OS migration'], 'scores': [0.09239432960748672, 0.08663392812013626, 0.08518557995557785, 0.08518557995557785, 0.08386624604463577, 0.07550071179866791, 0.06959494203329086, 0.06867493689060211, 0.0630602315068245, 0.06090902164578438, 0.05847746878862381, 0.05847746878862381, 0.057745154947042465, 0.054294440895318985]}, {'sequence': 'Applying Different Image Texture Material to Each Side of the Box in RealityKit\\n\\n\\nI am creating a box in RealityKit and want to apply different texture to each side of the box. I am using the following code but it always applies the first texture called \"lola\" on each of the side as a material. Am I missing something?\\n\\n\\ncancellable = TextureResource.loadAsync(named: \"lola\")\\n            .append(TextureResource.loadAsync(named: \"cover\"))\\n            .append(TextureResource.loadAsync(named: \"purple_flower\"))\\n            .append(TextureResource.loadAsync(named: \"cover\"))\\n            .append(TextureResource.loadAsync(named: \"purple_flower\"))\\n            .append(TextureResource.loadAsync(named: \"cover\"))\\n            \\n            .collect()\\n            .sink { [weak self] completion in\\n            if case let .failure(error) = completion {\\n                fatalError(\"Unable to load texture \\\\(error)\")\\n            }\\n            \\n            self?.cancellable?.cancel()\\n            \\n        } receiveValue: { textures in\\n            \\n            var materials: [UnlitMaterial] = []\\n            \\n            textures.forEach { texture in\\n                print(texture)\\n                var material = UnlitMaterial()\\n                material.color = .init(tint: .white, texture: .init(texture))\\n                materials.append(material)\\n            }\\n            \\n            box.model?.materials = materials\\n            anchor.addChild(box)\\n            arView.scene.addAnchor(anchor)\\n        }\\n\\n\\n\\n', 'labels': ['functionality implementation', 'API usage', 'framework comparison', 'device configuration', 'device deployment', 'version migration', 'plugin usage', 'plugin usage', 'software configuration', 'software configuration', 'runtime', 'math comprehension', 'OS configuration', 'OS migration'], 'scores': [0.11110372096300125, 0.08790700882673264, 0.08239290863275528, 0.07831890881061554, 0.07612115144729614, 0.0706653818488121, 0.0697232261300087, 0.0697232261300087, 0.06804193556308746, 0.06804193556308746, 0.06609941273927689, 0.05951956659555435, 0.05054483190178871, 0.04179682210087776]}, {'sequence': 'Is it possible to render XNA game over camera preview in windows phone 8\\n\\n\\nI have a demo working where I can render some text over a camera preview in windows phone 8, I now want to extend the app to render xna game content in an augmented reality style over camera preview.\\n\\n\\nIs this even possible with XNA?\\n\\n\\n', 'labels': ['functionality implementation', 'device configuration', 'software configuration', 'software configuration', 'runtime', 'device deployment', 'OS configuration', 'framework comparison', 'API usage', 'version migration', 'plugin usage', 'plugin usage', 'OS migration', 'math comprehension'], 'scores': [0.14227180182933807, 0.13496161997318268, 0.09849863499403, 0.09849863499403, 0.08083751052618027, 0.07734943181276321, 0.0668354257941246, 0.06029100343585014, 0.05871231108903885, 0.05060648173093796, 0.042576976120471954, 0.042576976120471954, 0.026336826384067535, 0.019646354019641876]}, {'sequence': 'C++ Depth Buffer Oculus Rift DK2 VR HMD\\n\\n\\nThis is a question for anybody experienced with the Oculus Rift C++ SDK.\\n\\n\\n\\n\\n\\nSDK Version: ovr_sdk_win_1.20.0_public\\n\\n\\nIDE: VS2013\\n\\n\\n\\n\\n\\nI am trying to resolve a depth buffer problem in my code for a simplified 3D engine for the Oculus Rift DK2. \\n\\n\\nThe OculusRoomTiny example supplied with the SDK is very complex as it is designed with versatility in mind. So, I\\'ve taken the SuperMinimal Sample code and used that as a basis for my engine. The SuperMinimal Sample did did not support multi-colour vertexes or depth buffers. So my code includes both.\\n\\n\\nI\\'ve had no problem creating depth buffers using Microsoft DirectX-11 libraries on PC apps so think it\\'s an Oculus specific challenge. Note: the OculusRoomTiny example depth buffer works fine but is encapsulated in classes too complex for my code.\\n\\n\\nIn my code, the Cube rendered as would be expected but inclusion of the Depth buffer only renders the world background colour.\\n\\n\\nI ripped the depth buffer code from the OculusRoomTiny_Advanced demo and am sure I have integrated it religiously.\\nI\\'ve posted my code for comment. Please note, it is super minimal and does not clean up the COM objects.\\n\\n\\nAny advice would be welcome as I\\'ve been playing with the code now for 6 weeks!\\n\\n\\nmain.cpp\\n\\n\\n#include \"d3d11.h\"\\n#include \"d3dcompiler.h\"\\n#include \"OVR_CAPI_D3D.h\"\\n#include \"DirectXMath.h\"\\n#include \"models.h\"\\n\\nusing namespace DirectX;\\n#pragma comment(lib, \"d3dcompiler.lib\")\\n#pragma comment(lib, \"dxgi.lib\")\\n#pragma comment(lib, \"d3d11.lib\")\\nvoid CreateSampleModel(ID3D11Device * Device, ID3D11DeviceContext * \\nContext);\\nvoid RenderSampleModel(XMMATRIX * viewProj, ID3D11Device * Device, ID3D11DeviceContext * Context);\\n\\nID3D11Buffer * IndexBuffer;\\nID3D11RenderTargetView * eyeRenderTexRtv[2][3];\\nID3D11DepthStencilView *zbuffer[2]; // containing one for each eye\\novrLayerEyeFov ld = { { ovrLayerType_EyeFov } };    //Only using one layer Update this to an array if using several\\novrSession session;\\nID3D11Device * Device;\\nID3D11DeviceContext * Context;\\n\\n\\nvoid CreateDepthBufferForBothEyes(int eye)\\n{\\n    // Create the Depth Buffer Texture\\n    D3D11_TEXTURE2D_DESC texd;\\n    ZeroMemory(&texd, sizeof(texd));\\n\\n    texd.Width = ld.Viewport[eye].Size.w;\\n    texd.Height = ld.Viewport[eye].Size.h;\\n    texd.ArraySize = 1;\\n    texd.MipLevels = 1;\\n    texd.SampleDesc.Count = 1; // This matches the RTV\\n    texd.Format = DXGI_FORMAT_D32_FLOAT;\\n    texd.BindFlags = D3D11_BIND_DEPTH_STENCIL;\\n    ID3D11Texture2D *pDepthBuffer;\\n    Device->CreateTexture2D(&texd, NULL, &pDepthBuffer);\\n\\n    // Describe specific properties of the Depth Stencil Buffer\\n    D3D11_DEPTH_STENCIL_VIEW_DESC dsvd;\\n    ZeroMemory(&dsvd, sizeof(dsvd));\\n\\n    dsvd.Format = DXGI_FORMAT_D32_FLOAT; // Make the same as the texture format\\n    dsvd.ViewDimension = D3D11_DSV_DIMENSION_TEXTURE2D;\\n\\n    Device->CreateDepthStencilView(pDepthBuffer, &dsvd, &zbuffer[eye]);\\n    pDepthBuffer->Release();\\n\\n}\\n\\n\\n//-------------------------------------------------------------------------------------------------\\nint WINAPI wWinMain(HINSTANCE hInstance, HINSTANCE, LPWSTR, int)\\n{\\n    // Init Rift and device\\n    ovr_Initialize(0);  \\n    ovrGraphicsLuid luid;\\n    ovr_Create(&session, &luid);\\n    IDXGIFactory * DXGIFactory; CreateDXGIFactory1(__uuidof(IDXGIFactory), (void**)(&DXGIFactory));\\n    IDXGIAdapter * DXGIAdapter;  DXGIFactory->EnumAdapters(0, &DXGIAdapter);\\n\\n    D3D11CreateDevice(DXGIAdapter, D3D_DRIVER_TYPE_UNKNOWN, 0, 0, 0, 0, D3D11_SDK_VERSION, &Device, 0, &Context);\\n\\n    // Create eye render buffers\\n    for (int eye = 0; eye < 2; eye++)\\n    {\\n        ld.Fov[eye] = ovr_GetHmdDesc(session).DefaultEyeFov[eye];\\n        ld.Viewport[eye].Size = ovr_GetFovTextureSize(session, (ovrEyeType)eye, ld.Fov[eye], 1.0f);\\n\\n        ovrTextureSwapChainDesc desc = {};\\n        desc.Type = ovrTexture_2D;\\n        desc.ArraySize = 1;\\n        desc.Format = OVR_FORMAT_R8G8B8A8_UNORM_SRGB;\\n        desc.Width = ld.Viewport[eye].Size.w;\\n        desc.Height = ld.Viewport[eye].Size.h;\\n        desc.MipLevels = 1;\\n        desc.SampleCount = 1;\\n        desc.StaticImage = ovrFalse;\\n        desc.MiscFlags = ovrTextureMisc_DX_Typeless;\\n        desc.BindFlags = ovrTextureBind_DX_RenderTarget;\\n\\n        ovr_CreateTextureSwapChainDX(session, Device, &desc, &ld.ColorTexture[eye]);\\n        int textureCount = 0; ovr_GetTextureSwapChainLength(session, ld.ColorTexture[eye], &textureCount);\\n\\n        for (int j = 0; j < textureCount; j++)  // Creates 3 backbuffers for each eye\\n        {\\n            ID3D11Texture2D* tex; ovr_GetTextureSwapChainBufferDX(session, ld.ColorTexture[eye], j, IID_PPV_ARGS(&tex));\\n            D3D11_RENDER_TARGET_VIEW_DESC rtvd = { DXGI_FORMAT_R8G8B8A8_UNORM, D3D11_RTV_DIMENSION_TEXTURE2D };\\n            Device->CreateRenderTargetView(tex, &rtvd, &eyeRenderTexRtv[eye][j]);\\n        }\\n        CreateDepthBufferForBothEyes(eye);\\n    }\\n\\n    // Create sample model to be rendered in VR\\n    CreateSampleModel(Device,Context);\\n\\n    // Loop for some frames, then terminate\\n    float camX = 0.0f;\\n    float camY = 0.0f;\\n    float camZ = 0.0f;\\n\\n    for (long long frameIndex = 0; frameIndex < 1000;)\\n    {\\n\\n        if (GetKeyState(VK_LEFT) & 0x8000)\\n            camX -= 0.001f;\\n        if (GetKeyState(VK_RIGHT) & 0x8000)\\n            camX += 0.001f;\\n\\n        if (GetKeyState(VK_UP) & 0x8000)\\n            camY += 0.001f;\\n        if (GetKeyState(VK_DOWN) & 0x8000)\\n            camY -= 0.001f;\\n\\n        if (GetKeyState(VK_NEXT) & 0x8000)\\n            camZ += 0.001f;\\n        if (GetKeyState(VK_PRIOR) & 0x8000)\\n            camZ -= 0.001f;\\n\\n        // Get pose using a default IPD\\n        ovrPosef HmdToEyePose[2] = {{{0,0,0,1}, {-0.032f, 0, 0}},\\n                                    {{0,0,0,1}, {+0.032f, 0, 0}}};\\n        ovrPosef pose[2]; ovr_GetEyePoses(session, 0, ovrTrue, HmdToEyePose, pose, &ld.SensorSampleTime);\\n\\n        //for (int eye = 0; eye < 2; eye++) \\n        //  ld.RenderPose[eye] = pose[eye]; // Update the Layer description with the new head position for each eye\\n\\n        // Render to each eye\\n        for (int eye = 0; eye < 2; eye++)\\n        {\\n            ld.RenderPose[eye] = pose[eye]; // Update the Layer description with the new head position for each eye\\n            // Set and clear current render target, and set viewport\\n            int index = 0;  ovr_GetTextureSwapChainCurrentIndex(session, ld.ColorTexture[eye], &index);\\n            Context->OMSetRenderTargets(1, &eyeRenderTexRtv[eye][index], zbuffer[eye]); // zbuffer[eye]\\n            Context->ClearRenderTargetView(eyeRenderTexRtv[eye][index], new float[]{ 0.1f, 0.0f, 0.0f, 0.0f });\\n            Context->ClearDepthStencilView(zbuffer[eye], D3D11_CLEAR_DEPTH | D3D11_CLEAR_STENCIL, 1.0, 0); \\n\\n            D3D11_VIEWPORT D3Dvp; // = { 0, 0, (float)ld.Viewport[eye].Size.w, (float)ld.Viewport[eye].Size.h };\\n            D3Dvp.TopLeftX = 0.0f;\\n            D3Dvp.TopLeftY = 0.0f;\\n            D3Dvp.Width = (float)ld.Viewport[eye].Size.w;\\n            D3Dvp.Height = (float)ld.Viewport[eye].Size.h;\\n            D3Dvp.MinDepth = 0;\\n            D3Dvp.MaxDepth = 1;\\n\\n            Context->RSSetViewports(1, &D3Dvp);\\n\\n            pose[eye].Position.z = 2.0f + camZ; // Move camera 2m from cube\\n            pose[eye].Position.x = camX;\\n            pose[eye].Position.y = camY;\\n            // Calculate view and projection matrices using pose and SDK\\n            XMVECTOR rot = XMLoadFloat4((XMFLOAT4 *)&pose[eye].Orientation);\\n            XMVECTOR pos = XMLoadFloat3((XMFLOAT3 *)&pose[eye].Position);\\n            XMVECTOR up = XMVector3Rotate(XMVectorSet(0, 1, 0, 0), rot);\\n            XMVECTOR forward = XMVector3Rotate(XMVectorSet(0, 0, 1, 0), rot);\\n            XMMATRIX view = XMMatrixLookAtLH(pos, XMVectorAdd(pos, forward), up);\\n            ovrMatrix4f p = ovrMatrix4f_Projection(ld.Fov[eye], 0, 1, ovrProjection_None);\\n            XMMATRIX proj = XMMatrixTranspose(XMLoadFloat4x4((XMFLOAT4X4 *)&p)); \\n\\n            // Render model and commit frame\\n            RenderSampleModel(&XMMatrixMultiply(view, proj), Device, Context);\\n            ovr_CommitTextureSwapChain(session, ld.ColorTexture[eye]);\\n        }\\n\\n\\n\\n        // Send rendered eye buffers to HMD, and increment the frame if we\\'re visible\\n        ovrLayerHeader* layers[1] = { &ld.Header };\\n        if (ovrSuccess == ovr_SubmitFrame(session, 0, nullptr, layers, 1)) \\n            frameIndex; // was frameIndex++; but changed to loop forever\\n    }\\n\\n    ovr_Shutdown();\\n}\\n\\n\\n\\n\\n//---------------------------------------------------------------------------------------\\n// THIS CODE IS NOT SPECIFIC TO VR OR THE SDK, JUST USED TO DRAW SOMETHING IN VR\\n//---------------------------------------------------------------------------------------\\nvoid CreateSampleModel(ID3D11Device * Device, ID3D11DeviceContext * Context)\\n{\\n    // Create Vertex Buffer\\n    //#define V(n) (n&1?+1.0f:-1.0f), (n&2?-1.0f:+1.0f), (n&4?+1.0f:-1.0f) \\n    //float vertices[] = { V(0), V(3), V(2), V(6), V(3), V(7), V(4), V(2), V(6), V(1), V(5), V(3), V(4), V(1), V(0), V(5), V(4), V(7) };\\n\\n    D3D11_BUFFER_DESC vbDesc = { sizeof(VERTEX) * NUM_OF_VERTICES, D3D11_USAGE_DEFAULT, D3D11_BIND_VERTEX_BUFFER };\\n    D3D11_SUBRESOURCE_DATA initData = { tList };\\n    ID3D11Buffer* VertexBuffer;  \\n    Device->CreateBuffer(&vbDesc, &initData, &VertexBuffer);\\n\\n\\n    //create the index buffer\\n    D3D11_BUFFER_DESC bd;\\n    bd.Usage = D3D11_USAGE_DYNAMIC;\\n    bd.ByteWidth = sizeof(short) * NUM_OF_INDICES;    // 3 per triangle, 12 triangles\\n\\n    bd.BindFlags = D3D11_BIND_INDEX_BUFFER;\\n    bd.CPUAccessFlags = D3D11_CPU_ACCESS_WRITE;\\n    bd.MiscFlags = 0;\\n    Device->CreateBuffer(&bd, NULL, &IndexBuffer);\\n    D3D11_MAPPED_SUBRESOURCE ms;\\n    Context->Map(IndexBuffer, NULL, D3D11_MAP_WRITE_DISCARD, NULL, &ms);      // map the buffer\\n    memcpy(ms.pData, indexes, NUM_OF_INDICES * sizeof(short));                              // copy the data\\n    Context->Unmap(IndexBuffer, NULL);\\n\\n\\n    // Create Vertex Shader\\n    char* vShader = \"float4x4 m;\"\\n\\n        \"struct VOut { \"\\n        \"   float4 position : SV_POSITION;\"\\n        \"   float4 color : COLOR;\"\\n        \"}; \"\\n\\n        \"VOut VS(float4 p1 : POSITION, float4 colour: COLOR)\"\\n        \"{\"\\n        \"   VOut output;\"\\n        \"   output.position = mul(m, p1);\"\\n        \"   output.color = colour;\"\\n        \"   return output;\"\\n        \"}\";\\n    ID3D10Blob * pBlob; D3DCompile(vShader, strlen(vShader), \"VS\", 0, 0, \"VS\", \"vs_4_0\", 0, 0, &pBlob, 0);\\n    ID3D11VertexShader * VertexShader; \\n    Device->CreateVertexShader(pBlob->GetBufferPointer(), pBlob->GetBufferSize(), 0, &VertexShader);\\n\\n    // Create Input Layout\\n    D3D11_INPUT_ELEMENT_DESC elements[] = { \\n        { \"POSITION\", 0, DXGI_FORMAT_R32G32B32_FLOAT, 0, 0, D3D11_INPUT_PER_VERTEX_DATA, 0 },\\n        { \"COLOR\", 0, DXGI_FORMAT_B8G8R8A8_UNORM, 0, 12, D3D11_INPUT_PER_VERTEX_DATA, 0 },\\n\\n    };\\n    ID3D11InputLayout  * InputLayout; \\n    Device->CreateInputLayout(elements, 2, pBlob->GetBufferPointer(), pBlob->GetBufferSize(), &InputLayout);\\n\\n    // Create Pixel Shader\\n    char* pShader = \"float4 PS(float4 position : POSITION, float4 colour : COLOR) : SV_Target { return colour; }\";\\n    D3DCompile(pShader, strlen(pShader), \"PS\", 0, 0, \"PS\", \"ps_4_0\", 0, 0, &pBlob, 0);\\n    ID3D11PixelShader  * PixelShader; \\n    Device->CreatePixelShader(pBlob->GetBufferPointer(), pBlob->GetBufferSize(), 0, &PixelShader);\\n\\n\\n    Context->IASetInputLayout(InputLayout);\\n    Context->IASetIndexBuffer(IndexBuffer, DXGI_FORMAT_R16_UINT, 0);\\n    UINT stride = sizeof(float) * 4U;   // 7U\\n    UINT offset = 0;\\n    Context->IASetVertexBuffers(0, 1, &VertexBuffer, &stride, &offset);\\n    Context->IASetPrimitiveTopology(D3D11_PRIMITIVE_TOPOLOGY_TRIANGLELIST);\\n    Context->VSSetShader(VertexShader, 0, 0);\\n    Context->PSSetShader(PixelShader, 0, 0);\\n}\\n//------------------------------------------------------\\nvoid RenderSampleModel(XMMATRIX * viewProj, ID3D11Device * Device, ID3D11DeviceContext * Context)\\n{\\n    D3D11_BUFFER_DESC desc = { sizeof(XMMATRIX), D3D11_USAGE_DYNAMIC, D3D11_BIND_CONSTANT_BUFFER, D3D11_CPU_ACCESS_WRITE };\\n    D3D11_SUBRESOURCE_DATA initData = { viewProj };\\n    ID3D11Buffer * ConstantBuffer;  Device->CreateBuffer(&desc, &initData, &ConstantBuffer); \\n    Context->VSSetConstantBuffers(0, 1, &ConstantBuffer);\\n    Context->DrawIndexed(NUM_OF_INDICES, 0, 0);\\n}\\n\\n\\n\\nmodels.h\\n\\n\\n#ifndef MODELS_H\\n#define MODELS_H\\n\\n#include \"DirectXMath.h\"\\n\\nusing namespace DirectX;\\n\\n#define NUM_OF_MODELS 1\\n\\nstruct VERTEX{\\n    float x;\\n    float y;\\n    float z;\\n    uint32_t  C;    // Colour\\n};\\n\\n#define NUM_OF_VERTICES 24\\n#define NUM_OF_INDICES 36\\n\\nextern VERTEX tList[];\\nextern short indexes[];\\n#endif\\n\\n\\n\\nmodels.cpp\\n\\n\\n#include \"models.h\"\\n\\nVERTEX tList[] = {\\n    { -0.1f, 0.1f, 0.2f, 0xFF0000FF },      // Cube Vertex Index 0 \\n    { 0.1f, 0.1f, 0.2f, 0xFF0000FF },       // 1 \\n    { -0.1f, -0.1f, 0.2f, 0xFF0000FF },     // 2 \\n    { 0.1f, -0.1f, 0.2f, 0xFF0000FF },      // 3 \\n\\n    { -0.1f, 0.1f, -0.0f, 0x00FF00FF },     // 4 \\n    { 0.1f, 0.1f, -0.0f, 0x00FF00FF },      // 5 \\n    { -0.1f, -0.1f, -0.0f, 0x00FF00FF },    // 6 \\n    { 0.1f, -0.1f, -0.0f, 0x00FF00FF },     // 7 \\n\\n    { 0.1f, 0.1f, 0.2f, 0x0000FFFF },       // 8\\n    { 0.1f, 0.1f, -0.0f, 0x0000FFFF },      // 9\\n    { 0.08f, -0.1f, 0.2f, 0x0000FFFF },     // 10 Distorted in prep to test Depth\\n    { 0.08f, -0.1f, -0.0f, 0x0000FFFF },    // 11 Distorted in prep to test Depth\\n\\n    { -0.1f, 0.1f, 0.2f, 0xFF0000FF },      // 12\\n    { -0.1f, 0.1f, -0.0f, 0xFF0000FF },     // 13\\n    { -0.1f, -0.1f, 0.2f, 0xFF0000FF },     // 14\\n    { -0.1f, -0.1f, -0.0f, 0xFF0000FF },        // 15 \\n\\n    { -0.1f, 0.1f, -0.0f, 0x00FF00FF },     // 16\\n    { 0.1f, 0.1f, -0.0f, 0x00FF00FF },      // 17\\n    { -0.1f, 0.1f, 0.2f, 0x00FF00FF },      // 18\\n    { 0.1f, 0.1f, 0.2f, 0x00FF00FF },       // 19 \\n\\n    { -0.1f, -0.1f, -0.0f, 0x00FFFFFF },        // 20\\n    { 0.1f, -0.1f, -0.0f, 0x00FFFFFF },     // 21\\n    { -0.1f, -0.1f, 0.2f, 0x00FFFFFF },     // 22\\n    { 0.1f, -0.1f, 0.2f, 0x00FFFFFF },      // 23 \\n\\n\\n};\\n\\nshort indexes[] = {\\n    0, 1, 2,    // FRONT QUAD \\n    2, 1, 3,\\n    5, 4, 6,    // BACK QUAD\\n    5, 6, 7,\\n    8, 9, 11,   // RIGHT QUAD\\n    8, 11, 10,\\n    13, 12,14,  // LEFT QUAD\\n    13, 14, 15,\\n    18,16, 17,  // TOP QUAD\\n    18, 17, 19,\\n    22, 23, 21, // BOTTOM QUAD\\n    22, 21, 20\\n};\\n\\n\\n\\n', 'labels': ['runtime', 'API usage', 'version migration', 'software configuration', 'software configuration', 'device deployment', 'device configuration', 'OS configuration', 'functionality implementation', 'plugin usage', 'plugin usage', 'OS migration', 'math comprehension', 'framework comparison'], 'scores': [0.07207953929901123, 0.07179823517799377, 0.0717843770980835, 0.07158894836902618, 0.07158894836902618, 0.07149854302406311, 0.07147708535194397, 0.07138733565807343, 0.07137857377529144, 0.07132948935031891, 0.07132948935031891, 0.07110773772001266, 0.07083069533109665, 0.07082100957632065]}, {'sequence': 'RealityKit – What is `steelBox` instance loading?\\n\\n\\nWhen you create an Augmented Reality Project using the standard Xcode template, Xcode adds this swift file to your project:\\n\\n\\n//\\n// Experience.swift\\n// GENERATED CONTENT. DO NOT EDIT.\\n//\\n\\nimport Foundation\\nimport RealityKit\\nimport simd\\nimport Combine\\n\\npublic enum Experience {\\n\\n    public enum LoadRealityFileError: Error {\\n        case fileNotFound(String)\\n    }\\n\\n    private static var streams = [Combine.AnyCancellable]()\\n\\n    public static func loadBox() throws -> Experience.Box {\\n        guard let realityFileURL = Foundation.Bundle(for: Experience.Box.self).url(forResource: \"Experience\", withExtension: \"reality\") else {\\n            throw Experience.LoadRealityFileError.fileNotFound(\"Experience.reality\")\\n        }\\n\\n        let realityFileSceneURL = realityFileURL.appendingPathComponent(\"Box\", isDirectory: false)\\n        let anchorEntity = try Experience.Box.loadAnchor(contentsOf: realityFileSceneURL)\\n        return createBox(from: anchorEntity)\\n    }\\n\\n    public static func loadBoxAsync(completion: @escaping (Swift.Result<Experience.Box, Swift.Error>) -> Void) {\\n        guard let realityFileURL = Foundation.Bundle(for: Experience.Box.self).url(forResource: \"Experience\", withExtension: \"reality\") else {\\n            completion(.failure(Experience.LoadRealityFileError.fileNotFound(\"Experience.reality\")))\\n            return\\n        }\\n\\n        var cancellable: Combine.AnyCancellable?\\n        let realityFileSceneURL = realityFileURL.appendingPathComponent(\"Box\", isDirectory: false)\\n        let loadRequest = Experience.Box.loadAnchorAsync(contentsOf: realityFileSceneURL)\\n        cancellable = loadRequest.sink(receiveCompletion: { loadCompletion in\\n            if case let .failure(error) = loadCompletion {\\n                completion(.failure(error))\\n            }\\n            streams.removeAll { $0 === cancellable }\\n        }, receiveValue: { entity in\\n            completion(.success(Experience.createBox(from: entity)))\\n        })\\n        cancellable?.store(in: &streams)\\n    }\\n\\n    private static func createBox(from anchorEntity: RealityKit.AnchorEntity) -> Experience.Box {\\n        let box = Experience.Box()\\n        box.anchoring = anchorEntity.anchoring\\n        box.addChild(anchorEntity)\\n        return box\\n    }\\n\\n    public class Box: RealityKit.Entity, RealityKit.HasAnchoring {\\n\\n        public var steelBox: RealityKit.Entity? {\\n            return self.findEntity(named: \"Steel Box\")\\n        }\\n\\n    }\\n\\n}\\n\\n\\n\\nlet\\'s focus on the last part of the code\\n\\n\\npublic class Box: RealityKit.Entity, RealityKit.HasAnchoring {\\n    public var steelBox: RealityKit.Entity? {\\n        return self.findEntity(named: \"Steel Box\")\\n    }\\n}\\n\\n\\n\\nthis part is apparently loading the cube, named \"Steel Box\" from the standard Experience.rcproject. But If I rename this string to any random name that is not on the scene, the cube still loads fine and so does the project... and this method is called during initialization!\\n\\n\\nWhy is that? What is this really loading, or not?\\n\\n\\n', 'labels': ['functionality implementation', 'framework comparison', 'device configuration', 'API usage', 'device deployment', 'software configuration', 'software configuration', 'version migration', 'plugin usage', 'plugin usage', 'OS configuration', 'math comprehension', 'OS migration', 'runtime'], 'scores': [0.07544847577810287, 0.07342185825109482, 0.0730167031288147, 0.07299444824457169, 0.07290840148925781, 0.07183775305747986, 0.07183775305747986, 0.07113925367593765, 0.07104125618934631, 0.07104125618934631, 0.07003998756408691, 0.06892262399196625, 0.06823641806840897, 0.06811382621526718]}, {'sequence': \"How to create a bitmap from ViewRenderable / Node using SceneForm?\\n\\n\\n\\n\\n\\nI'm currently displaying the above image (black is transparent actually) over the augmented image, using Sceneform (Maintained repo by Thomas Gorisse, which is based on ARCore and filament).\\n\\n\\n\\n\\n\\nIs there any method by which I could screenshot only the ViewRenderable node displayed in Augmented Camera output ? I need the bitmap output of what is shown in the node, as exactly as shown in the output.\\n\\n\\n\", 'labels': ['functionality implementation', 'API usage', 'device configuration', 'software configuration', 'software configuration', 'framework comparison', 'device deployment', 'math comprehension', 'plugin usage', 'plugin usage', 'version migration', 'OS configuration', 'runtime', 'OS migration'], 'scores': [0.16137650609016418, 0.10018933564424515, 0.09058316797018051, 0.08908821642398834, 0.08908821642398834, 0.08896069973707199, 0.0723922848701477, 0.06278227269649506, 0.05146756395697594, 0.05146756395697594, 0.04869570583105087, 0.03600570186972618, 0.034825801849365234, 0.023077048361301422]}, {'sequence': 'Rotate MeasuringNode along with phone\\n\\n\\n\\nI successfully achieve to add a node in front of camera. Now I want to rotate it as I move my phone\\n\\n\\n\\nSee youtube video here . I want to achieve that center rotation in my app. After calibrating Iphone the center node slightly tilt when phone is moved at an angle...\\n\\n\\nI have seen a IOS code.\\n\\n\\n    let camera = frame.camera.transform.vector3;\\n    self.centerNode.look(at: SCNVector3Make(camera.x, reticleNode.position.y, camera.z));\\n\\n\\n\\nSo I tried\\n\\n\\n     setLookDirection(Vector3(mCameraPose.tx(),it.qy(),mCameraPose.tz()))\\n\\n\\n\\nHow do I set my CenterAnchorNode to tilt as my phone tilt . I am replicating Iphone Measuring app to andorid.\\n\\n\\n', 'labels': ['OS configuration', 'device configuration', 'OS migration', 'functionality implementation', 'device deployment', 'software configuration', 'software configuration', 'math comprehension', 'API usage', 'framework comparison', 'runtime', 'version migration', 'plugin usage', 'plugin usage'], 'scores': [0.14860737323760986, 0.10108157247304916, 0.08794968575239182, 0.0811830684542656, 0.07637587189674377, 0.0744522213935852, 0.0744522213935852, 0.05659044533967972, 0.056085724383592606, 0.05506644397974014, 0.051271140575408936, 0.04615262150764465, 0.045365795493125916, 0.045365795493125916]}, {'sequence': 'How to place a single infinite plane in ARCore?\\n\\n\\nHow can I place a plane with infinite length using arcore.\\nI want to do exactly like this github discussion URL. \\nThe I found solution for unity ARCore sdk : link.\\n\\n\\nSo how can I achieve that using java?\\n\\n\\n', 'labels': ['functionality implementation', 'API usage', 'software configuration', 'software configuration', 'framework comparison', 'runtime', 'plugin usage', 'plugin usage', 'device configuration', 'math comprehension', 'device deployment', 'version migration', 'OS configuration', 'OS migration'], 'scores': [0.13279925286769867, 0.08932523429393768, 0.08102656155824661, 0.08102656155824661, 0.08043187111616135, 0.07367311418056488, 0.07313790917396545, 0.07313790917396545, 0.06651607155799866, 0.057029545307159424, 0.056437503546476364, 0.054032955318689346, 0.044588178396224976, 0.03683735802769661]}, {'sequence': 'Apple AR Kit feature points for entire session\\n\\n\\n\\nI understand how to extract feature points for a single ARFrame (ARFrame.rawFeaturePoints). Is there anyway to extract all feature points that have been detected in the session? Is this something I have to aggregate myself? If so, how should I handle point matching?\\n\\n\\n', 'labels': ['functionality implementation', 'software configuration', 'software configuration', 'device configuration', 'framework comparison', 'API usage', 'OS configuration', 'device deployment', 'plugin usage', 'plugin usage', 'runtime', 'version migration', 'math comprehension', 'OS migration'], 'scores': [0.1637972593307495, 0.11290747672319412, 0.11290747672319412, 0.10697399079799652, 0.08182070404291153, 0.0756194144487381, 0.055223241448402405, 0.05288812518119812, 0.04645165801048279, 0.04645165801048279, 0.04570618271827698, 0.03937127813696861, 0.031118175014853477, 0.028763391077518463]}]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model='cross-encoder/nli-distilroberta-base')\n",
    "\n",
    "res = classifier(docs, steps)\n",
    "print(res)\n",
    "\n",
    "with open('roberta_steps.json',  'w') as fj:\n",
    "    json.dump(res, fj, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arstudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a526fedd6feb1353d847e80cd7d79ee8a9def4aa5fbe789cada2f9cc63832371"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
